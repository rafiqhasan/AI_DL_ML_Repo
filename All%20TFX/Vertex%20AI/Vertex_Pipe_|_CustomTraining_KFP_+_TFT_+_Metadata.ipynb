{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vertex Pipe | CustomTraining  KFP + TFT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOC6cWtm4e3eDHgtRnfy2m9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafiqhasan/AI_DL_ML_Repo/blob/master/All%2520TFX/Vertex%2520AI/Vertex_Pipe_%7C_CustomTraining_KFP_%2B_TFT_%2B_Metadata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K6hewFD3rb1d",
        "outputId": "314df7b6-f4cf-4209-bc54-d77982b8488d"
      },
      "source": [
        "!pip install kfp\n",
        "!pip install google-cloud-pipeline-components\n",
        "!pip install -q -U tensorflow_transform"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kfp\n",
            "  Downloading kfp-1.7.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting absl-py<=0.11,>=0.9\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML<6,>=5.3\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<2,>=1.20.0\n",
            "  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 78.3 MB/s \n",
            "\u001b[?25hCollecting kubernetes<13,>=8.0.0\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from kfp) (1.12.8)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from kfp) (1.34.0)\n",
            "Collecting requests-toolbelt<1,>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from kfp) (1.3.0)\n",
            "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
            "  Downloading kfp-server-api-1.6.0.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/dist-packages (from kfp) (0.8.9)\n",
            "Requirement already satisfied: click<8,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from kfp) (7.1.2)\n",
            "Collecting Deprecated<2,>=1.2.7\n",
            "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
            "Collecting strip-hints<1,>=0.1.8\n",
            "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
            "Collecting docstring-parser<1,>=0.7.3\n",
            "  Downloading docstring_parser-0.10.tar.gz (20 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kfp-pipeline-spec<0.2.0,>=0.1.8\n",
            "  Downloading kfp_pipeline_spec-0.1.9-py3-none-any.whl (17 kB)\n",
            "Collecting fire<1,>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from kfp) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<=0.11,>=0.9->kfp) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.17.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2018.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.7.2)\n",
            "Collecting google-api-core<2dev,>=1.21.0\n",
            "  Downloading google_api_core-1.31.2-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.0.0-py2.py3-none-any.whl (27 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.0-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (2.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (4.6.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.0)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=8.0.0->kfp) (1.3.0)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp) (3.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp) (3.1.1)\n",
            "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-1.7.1-py3-none-any.whl size=321909 sha256=c33a38a6104a04e2285404aa4f5d6ff16e0b74fa296ab1f4dc2497d490167df5\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/7e/d3/c3958e65cc9aa628c9398eb0ee17c804c3934f1111f17310b8\n",
            "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docstring-parser: filename=docstring_parser-0.10-py3-none-any.whl size=28862 sha256=08884323da50755c0601eb470d99965b8d0ddb7f81daeb5dc0906f0c84eaafbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/12/f3/67c96229e15e2fe18709cba1b9e3a6a982699c4f0a7b4c0748\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=6d37da336449511bf0f67f506b54445d7cf4851a1151969a846fcca0a4257fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-1.6.0-py3-none-any.whl size=92524 sha256=670b748e4cf5413a82d32971cd78649ce7706e61d1829713913748ce29fca600\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/2f/7c/d5c1cbcb535e30c90b88aa5104b1ee14a0ea9313a543bfdf52\n",
            "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=6b3c0e7b88b9f81879d32019bcf571052d2bddaebc0484ae28529529f0be17b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
            "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
            "Installing collected packages: google-crc32c, google-api-core, websocket-client, PyYAML, google-resumable-media, google-cloud-core, strip-hints, requests-toolbelt, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-cloud-storage, fire, docstring-parser, Deprecated, absl-py, kfp\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.4 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.12 PyYAML-5.4.1 absl-py-0.11.0 docstring-parser-0.10 fire-0.4.0 google-api-core-1.31.2 google-cloud-core-2.0.0 google-cloud-storage-1.42.0 google-crc32c-1.1.2 google-resumable-media-2.0.0 jsonschema-3.2.0 kfp-1.7.1 kfp-pipeline-spec-0.1.9 kfp-server-api-1.6.0 kubernetes-12.0.1 requests-toolbelt-0.9.1 strip-hints-0.1.10 websocket-client-1.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-pipeline-components\n",
            "  Downloading google_cloud_pipeline_components-0.1.5-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting google-cloud-aiplatform>=1.3.0\n",
            "  Downloading google_cloud_aiplatform-1.3.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core<2dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pipeline-components) (1.31.2)\n",
            "Requirement already satisfied: kfp<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pipeline-components) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (21.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.34.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (0.2.8)\n",
            "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (1.42.0)\n",
            "Collecting proto-plus>=1.10.1\n",
            "  Downloading proto_plus-1.19.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (1.21.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.39.0)\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1\n",
            "  Downloading google_resumable_media-0.4.1-py2.py3-none-any.whl (38 kB)\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 61.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 77.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 82.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 76.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 76.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 75.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 73.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 12.0 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.24.1-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (2.0.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (2.0.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components) (2.20)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.3.0)\n",
            "Requirement already satisfied: fire<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.4.0)\n",
            "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.6.0)\n",
            "Requirement already satisfied: jsonschema<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (3.2.0)\n",
            "Requirement already satisfied: PyYAML<6,>=5.3 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (5.4.1)\n",
            "Requirement already satisfied: kubernetes<13,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (12.0.1)\n",
            "Requirement already satisfied: click<8,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (7.1.2)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.10)\n",
            "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.1.9)\n",
            "Requirement already satisfied: absl-py<=0.11,>=0.9 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.11.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.12.8)\n",
            "Requirement already satisfied: strip-hints<1,>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.1.10)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.9.1)\n",
            "Requirement already satisfied: Deprecated<2,>=1.2.7 in /usr/local/lib/python3.7/dist-packages (from kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.2.12)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated<2,>=1.2.7->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<1,>=0.3.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.1.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (21.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (4.6.4)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.18.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (2021.5.30)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=8.0.0->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.2.1)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=8.0.0->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.10)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from strip-hints<1,>=0.1.8->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (0.37.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp<2.0.0,>=1.4.0->google-cloud-pipeline-components) (3.1.1)\n",
            "Installing collected packages: proto-plus, google-cloud-bigquery, google-cloud-aiplatform, google-cloud-pipeline-components\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.24.1 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-aiplatform-1.3.0 google-cloud-bigquery-2.24.1 google-cloud-pipeline-components-0.1.5 proto-plus-1.19.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 406 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 37.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 189 kB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.0 MB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.7 MB 74 kB/s \n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 9.4 kB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 57.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 883 kB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 41.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 183 kB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 63.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 169 kB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 267 kB 53.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 435 kB 64.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 255 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 61.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 48.9 MB/s \n",
            "\u001b[?25h  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-cloud-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.20.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHIDChoqpOF0"
      },
      "source": [
        "!rm -rf /content/task.py"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7mXFi3UwTzY",
        "outputId": "fc2de000-2f89-4502-e791-c48902b6bcba"
      },
      "source": [
        "%%writefile -a task.py\n",
        "\n",
        "# Owner - Hasan Rafiq\n",
        "# Load the TensorBoard notebook extension\n",
        "\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "import apache_beam as beam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import folium\n",
        "import tempfile\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from folium import plugins\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# https://www.tensorflow.org/tfx/tutorials/transform/census\n",
        "def tf_preprocessing_fn(inputs):\n",
        "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "    pickup_latitude = inputs['pickup_latitude']\n",
        "    pickup_longitude = inputs['pickup_longitude']\n",
        "\n",
        "    #Calculate unique values of attribute\n",
        "    tft.vocabulary(inputs['trip_start_day'], vocab_filename='trip_start_day')\n",
        "\n",
        "    pickup_latitude_scaled = tft.scale_to_0_1(pickup_latitude)\n",
        "    pickup_longitude_scaled = tft.scale_to_0_1(pickup_longitude)\n",
        "    distance = ((inputs['pickup_latitude'] - inputs['dropoff_latitude'])**2 +  (inputs['pickup_longitude'] - inputs['dropoff_longitude'])**2)**0.5\n",
        "\n",
        "    return {\n",
        "        'pickup_latitude_scaled': pickup_latitude_scaled,\n",
        "        'pickup_longitude_scaled': pickup_longitude_scaled,\n",
        "        'distance': distance,\n",
        "        # 'dummy1': dummy1\n",
        "    }\n",
        "\n",
        "def beam_run_transform_data(train_data_file, transform_fn_output_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Read in the data using the CSV reader, and transform it using a\n",
        "  preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  from strings to int64 values indices, by creating a vocabulary for each\n",
        "  category.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    transform_fn_output_dir: Directory to write function\n",
        "  \"\"\"\n",
        " \n",
        "  SCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "    tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)).schema\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a TFXIO to read the census data with the schema. To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "      # accepts a PCollection[bytes] because we need to patch the records first\n",
        "      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "      # to both read the CSV files and parse them to TFT inputs:\n",
        "      # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "      csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "          physical_format='text',\n",
        "          column_names=CSV_COLUMNS,\n",
        "          schema=SCHEMA)\n",
        "\n",
        "      # Read in raw data and convert using CSV TFXIO.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do the from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV TFXIO can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(\n",
        "              train_data_file, coder=beam.coders.BytesCoder())\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(b', ', b','))\n",
        "          | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "      # The TFXIO output format is chosen for improved performance.\n",
        "      transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeDataset(tf_preprocessing_fn))\n",
        "\n",
        "      # Will write a SavedModel and metadata to transform_fn_output_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(transform_fn_output_dir))\n",
        "  \n",
        "###############################\n",
        "##Feature engineering functions\n",
        "def feature_engg_features(features):\n",
        "  #Add TFT transformations\n",
        "  raw_features = {}\n",
        "  for key, val in features.items():\n",
        "    if key not in RAW_DATA_FEATURE_SPEC:\n",
        "      continue\n",
        "    if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
        "      raw_features[key] = tf.RaggedTensor.from_tensor(\n",
        "          tf.expand_dims(val, -1)).to_sparse()\n",
        "      continue\n",
        "    raw_features[key] = val\n",
        "  tft_new_features = tft_layer(raw_features)\n",
        "\n",
        "  features['pickup_longitude_scaled'] = tft_new_features['pickup_longitude_scaled'] \n",
        "  features['pickup_latitude_scaled'] = tft_new_features['pickup_latitude_scaled']\n",
        "  features['distance'] = tft_new_features['distance']\n",
        "  # features['dummy1'] = tft_new_features['dummy1']\n",
        "  \n",
        "  #Add new features( Non-TFT transformation ) -> Just for study purposes\n",
        "  # features['distance'] = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5\n",
        "\n",
        "  return(features)\n",
        "\n",
        "#To be called from TF\n",
        "def feature_engg(features, label):\n",
        "  #Add new features\n",
        "  features = feature_engg_features(features)\n",
        "\n",
        "  return(features, label)\n",
        "\n",
        "###############################\n",
        "###Data Input pipeline function\n",
        "\n",
        "def make_input_fn(filename, mode, vnum_epochs = None, batch_size = 512):\n",
        "    def _input_fn(v_test=False):     \n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.io.gfile.glob(filename)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = vnum_epochs # indefinitely\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this        \n",
        "        \n",
        "        # Create dataset from file list\n",
        "        dataset = tf.compat.v1.data.experimental.make_csv_dataset(file_list,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   column_names=CSV_COLUMNS,\n",
        "                                                   column_defaults=DEFAULTS,\n",
        "                                                   label_name=LABEL_COLUMN,\n",
        "                                                   num_epochs = num_epochs,\n",
        "                                                   num_parallel_reads=30)\n",
        "        \n",
        "        dataset = dataset.prefetch(buffer_size = batch_size)\n",
        "\n",
        "        #Feature engineering\n",
        "        dataset = dataset.map(feature_engg)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = vnum_epochs # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(num_epochs)       \n",
        "        \n",
        "        #Begins - Uncomment for testing only -----------------------------------------------------<\n",
        "        if v_test == True:\n",
        "          print(next(dataset.__iter__()))\n",
        "          \n",
        "        #End - Uncomment for testing only -----------------------------------------------------<\n",
        "        return dataset\n",
        "    return _input_fn\n",
        "\n",
        "# Define feature columns(Including feature engineered ones )\n",
        "# These are the features which come from the TF Data pipeline\n",
        "def create_feature_cols():\n",
        "    #Keras format features\n",
        "    k_pickup_longitude_scaled = tf.keras.Input(name='pickup_longitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
        "    k_pickup_latitude_scaled = tf.keras.Input(name='pickup_latitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
        "    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string, sparse=False)\n",
        "    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string, sparse=False)\n",
        "    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string, sparse=False)\n",
        "    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
        "    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
        "    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
        "    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
        "    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32, sparse=False)\n",
        "    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n",
        "                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n",
        "                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance,\n",
        "                        'pickup_longitude_scaled': k_pickup_longitude_scaled,\n",
        "                        'pickup_latitude_scaled' : k_pickup_latitude_scaled\n",
        "                        }\n",
        "\n",
        "    return({'K' : keras_dict_input})\n",
        "\n",
        "def create_keras_model(params, feature_cols):\n",
        "    METRICS = [\n",
        "            keras.metrics.RootMeanSquaredError(name='rmse')\n",
        "    ]\n",
        "\n",
        "    #Input layers\n",
        "    input_feats = []\n",
        "    for inp in feature_cols['K'].keys():\n",
        "      input_feats.append(feature_cols['K'][inp])\n",
        "\n",
        "    ##Input processing\n",
        "    ##https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
        "    ##https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md\n",
        "\n",
        "    ##Handle categorical attributes( One-hot encoding )\n",
        "    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary = ['0'] + tf_transform_output.vocabulary_by_name(vocab_filename='trip_start_day'), mask_token=None, oov_token = '0')(feature_cols['K']['trip_start_day'])\n",
        "    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=tf_transform_output.vocabulary_size_by_name(vocab_filename='trip_start_day') + 1)(cat_day)\n",
        "\n",
        "    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
        "                                                                                      '9','10','11','12','13','14','15','16',\n",
        "                                                                                      '17','18','19','20','21','22','23','0'\n",
        "                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n",
        "    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=24)(cat_hour)\n",
        "\n",
        "    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
        "                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n",
        "    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=12)(cat_month)\n",
        "\n",
        "    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n",
        "    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=len(df['company'].unique()))(cat_company)\n",
        "\n",
        "    ##Binning\n",
        "    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n",
        "    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n",
        "\n",
        "    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n",
        "    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n",
        "\n",
        "    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n",
        "    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n",
        "\n",
        "    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n",
        "    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n",
        "\n",
        "    ##Categorical cross\n",
        "    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n",
        "    hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=24 * 8)(cross_day_hour)\n",
        "    cat_cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = 24 * 8)(hash_cross_day_hour)\n",
        "\n",
        "    cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n",
        "    hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n",
        "\n",
        "    cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n",
        "    hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n",
        "\n",
        "    # Cross to embedding\n",
        "    embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n",
        "    embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n",
        "\n",
        "    embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n",
        "    embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n",
        "\n",
        "    # Also pass time attributes as Deep signal( Cast to integer )\n",
        "    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n",
        "    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n",
        "    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n",
        "\n",
        "    #Add feature engineered columns - LAMBDA layer\n",
        "\n",
        "    ###Create MODEL\n",
        "    ####Concatenate all features( Numerical input )\n",
        "    x_input_numeric = tf.keras.layers.concatenate([\n",
        "                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n",
        "                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n",
        "                    feature_cols['K']['pickup_latitude_scaled'], feature_cols['K']['pickup_longitude_scaled'],\n",
        "                    feature_cols['K']['distance'], embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n",
        "                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n",
        "                    ])\n",
        "\n",
        "    #DEEP - This Dense layer connects to input layer - Numeric Data\n",
        "    x_numeric = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_numeric)\n",
        "    x_numeric = tf.keras.layers.BatchNormalization()(x_numeric)\n",
        "\n",
        "    ####Concatenate all Categorical features( Categorical converted )\n",
        "    x_input_categ = tf.keras.layers.concatenate([\n",
        "                    cat_month, cat_cross_day_hour, cat_pickup_lat, cat_pickup_lon,\n",
        "                    cat_drop_lat, cat_drop_lon\n",
        "                    ])\n",
        "    \n",
        "    #WIDE - This Dense layer connects to input layer - Categorical Data\n",
        "    x_categ = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")(x_input_categ)\n",
        "\n",
        "    ####Concatenate both Wide and Deep layers\n",
        "    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n",
        "\n",
        "    for l_ in range(params['hidden_layers']):\n",
        "        x = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\",\n",
        "                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    #Final Layer\n",
        "    out = tf.keras.layers.Dense(1, activation='relu')(x)\n",
        "    model = tf.keras.Model(input_feats, out)\n",
        "\n",
        "    #Set optimizer\n",
        "    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \n",
        "                                        beta_2=params['beta_2'], epsilon=params['epsilon'])\n",
        "\n",
        "    #Compile model\n",
        "    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n",
        "\n",
        "    #Print Summary\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n",
        "  #Add callbacks\n",
        "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=5, min_lr=0.00001, verbose = 1)\n",
        "  \n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\")\n",
        "\n",
        "  #Train and Evaluate\n",
        "  out = model.fit(train_dataset, \n",
        "                  validation_data = validation_dataset,\n",
        "                  epochs=epochs,\n",
        "                  # validation_steps = 3,   ###Keep this none for running evaluation on full EVAL data every epoch\n",
        "                  steps_per_epoch = 100,   ###Has to be passed - Cant help it :) [ Number of batches per epoch ]\n",
        "                  callbacks=[reduce_lr, #modelsave_callback, \n",
        "                             tensorboard_callback, \n",
        "                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
        "                  )\n",
        "  \n",
        "  return (min(out.history['val_rmse']))\n",
        "  \n",
        "@tf.function\n",
        "def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n",
        "    #Params coming in request\n",
        "    features = {\n",
        "        'dropoff_latitude': dropoff_latitude,\n",
        "        'dropoff_longitude': dropoff_longitude,\n",
        "        'pickup_latitude': pickup_latitude,\n",
        "        'pickup_longitude': pickup_longitude,\n",
        "        'trip_start_day': trip_start_day,\n",
        "        'trip_start_hour': trip_start_hour,\n",
        "        'trip_start_month': trip_start_month\n",
        "    }\n",
        "\n",
        "    #Add TFT transformations\n",
        "    raw_features = {}\n",
        "    for key, val in features.items():\n",
        "      if key not in RAW_DATA_FEATURE_SPEC:\n",
        "        continue\n",
        "      if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
        "        raw_features[key] = tf.RaggedTensor.from_tensor(\n",
        "            tf.expand_dims(val, -1)).to_sparse()\n",
        "        continue\n",
        "      raw_features[key] = val\n",
        "    tft_new_features = tft_layer(raw_features)\n",
        "\n",
        "    pickup_longitude_scaled = tft_new_features['pickup_longitude_scaled'] \n",
        "    pickup_latitude_scaled = tft_new_features['pickup_latitude_scaled']\n",
        "    distance = tft_new_features['distance']\n",
        "\n",
        "    ##Feature engineering( calculate distance )\n",
        "    # distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n",
        "\n",
        "    #Params in request + New Feature engineering params\n",
        "    payload = {\n",
        "        'dropoff_latitude': dropoff_latitude,\n",
        "        'dropoff_longitude': dropoff_longitude,\n",
        "        'pickup_latitude': pickup_latitude,\n",
        "        'pickup_longitude': pickup_longitude,\n",
        "        'trip_start_day': trip_start_day,\n",
        "        'trip_start_hour': trip_start_hour,\n",
        "        'trip_start_month': trip_start_month,\n",
        "        'distance': distance,\n",
        "        'pickup_longitude_scaled': pickup_longitude_scaled,\n",
        "        'pickup_latitude_scaled': pickup_latitude_scaled,\n",
        "    }\n",
        "    \n",
        "    ## Predict\n",
        "    ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, \"COULDN'T COMPUTE OUTPUT TENSOR\"\n",
        "    predictions = m_(payload)\n",
        "    return predictions"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQJkYyxgrfRu"
      },
      "source": [
        "# !rm -rf /content/transform_fn\n",
        "# !rm -rf /content/*.csv"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNkdbqsYvojo"
      },
      "source": [
        "# !gsutil cp gs://rafiqhasandttl_01/taxi_dataset/train.csv /content/train.csv\n",
        "# !gsutil cp gs://rafiqhasandttl_01/taxi_dataset/eval.csv /content/eval.csv"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpm_c-H77t4V"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ky3V9NkzFkW",
        "outputId": "39d56e5b-d2c9-4f56-f638-6fd851030f67"
      },
      "source": [
        "%%writefile -a task.py\n",
        "\n",
        "##MAIN runner\n",
        "print(tf.__version__)\n",
        "\n",
        "device = \"cpu\"\n",
        "if device == \"tpu\":\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "else:\n",
        "  strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "#Constants\n",
        "train_file = 'gs://rafiqhasandttl_01/taxi_dataset/train.csv' ##\"/content/train.csv\"\n",
        "eval_file = 'gs://rafiqhasandttl_01/taxi_dataset/eval.csv' ##\"/content/eval.csv\"\n",
        "transform_fn_output_dir= 'gs://rafiqhasandttl_01/ml_temporary/taxi_dataset/transform_fn'\n",
        "model_save_location='gs://rafiqhasandttl_01/ml_models/taxi_dataset/'\n",
        "epochs=3\n",
        "\n",
        "# Determine CSV, label, and key columns\n",
        "#Columns in training sheet -> Can have extra columns too\n",
        "CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
        "       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "       'dropoff_longitude']\n",
        "LABEL_COLUMN = 'fare'\n",
        "\n",
        "# Set default values for each CSV column( Including Y column )\n",
        "DEFAULTS = [[0.0], ['1'], ['1'],['1'],[0.0],[0.0],[0.0],[0.0]]\n",
        "\n",
        "bins_lat = [41.66367065, 41.85934972, 41.87740612, 41.87925508, 41.88099447,\n",
        "       41.88498719, 41.88530002, 41.89204214, 41.89207263, 41.89265811,\n",
        "       41.89830587, 41.89960211, 41.90026569, 41.90741282, 41.92187746,\n",
        "       41.92926299, 41.9442266 , 41.95402765, 41.97907082, 42.02122359]\n",
        "\n",
        "bins_lon = [-87.9136246 , -87.76550161, -87.68751552, -87.67161972,\n",
        "       -87.66341641, -87.65599818, -87.65253448, -87.64629348,\n",
        "       -87.642649  , -87.63784421, -87.63330804, -87.63274649,\n",
        "       -87.63186395, -87.62887416, -87.62621491, -87.62519214,\n",
        "       -87.62099291, -87.62076287, -87.61886836, -87.54093551]\n",
        "\n",
        "RAW_DATA_FEATURE_SPEC = dict([\n",
        "        ('fare', tf.io.VarLenFeature(tf.float32)),\n",
        "        ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
        "        ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
        "        ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
        "        ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ])\n",
        "\n",
        "#Run TFT Transformation\n",
        "beam_run_transform_data(train_data_file=train_file, transform_fn_output_dir=transform_fn_output_dir)\n",
        "\n",
        "#Create TFT layer\n",
        "tf_transform_output = tft.TFTransformOutput(transform_fn_output_dir)\n",
        "tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "#Create FC\n",
        "create_feature_cols()\n",
        "\n",
        "#Create model\n",
        "params_default = {\n",
        "    'lr' : 0.001,\n",
        "    'beta_1' : 0.99,\n",
        "    'beta_2' : 0.999,\n",
        "    'epsilon' : 1e-08,\n",
        "    'decay' : 0.01,\n",
        "    'hidden_layers' : 1\n",
        "}\n",
        "\n",
        "#Create dataset input functions\n",
        "train_dataset = make_input_fn(filename = train_file,\n",
        "                    mode = tf.estimator.ModeKeys.TRAIN,\n",
        "                    batch_size = 128)()\n",
        "\n",
        "validation_dataset = make_input_fn(filename = eval_file,\n",
        "                    mode = tf.estimator.ModeKeys.EVAL,\n",
        "                    batch_size = 512)()\n",
        "\n",
        "m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols())\n",
        "tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n",
        "\n",
        "#Train Model\n",
        "rmse = keras_train_and_evaluate(m_, train_dataset, validation_dataset, epochs)\n",
        "print(\"Final Val RMSE: \", rmse)\n",
        "\n",
        "#Save model\n",
        "serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n",
        "                                        trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n",
        "                                        trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n",
        "                                        dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n",
        "                                        dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n",
        "                                        pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n",
        "                                        pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n",
        "                                        )\n",
        "\n",
        "version = \"1\"  #{'serving_default': call_output}\n",
        "tf.saved_model.save(\n",
        "    m_,\n",
        "    model_save_location + version,\n",
        "    signatures=serving\n",
        ") "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Appending to task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3C8Kh1xPkmx",
        "outputId": "882f8112-da3b-4147-ba05-7742ffcf16ca"
      },
      "source": [
        "!python task.py"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-24 16:13:52.825224: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2.5.1\n",
            "2021-08-24 16:13:54.348377: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-24 16:13:54.361983: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-08-24 16:13:54.362038: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0af8948a189e): /proc/driver/nvidia/version does not exist\n",
            "2021-08-24 16:13:54.362526: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "2021-08-24 16:13:55.625390: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-08-24 16:13:55.625863: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
            "2021-08-24 16:13:57.087835: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py:2382: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py:2382: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:max_tokens is deprecated, please use num_tokens instead.\n",
            "WARNING:tensorflow:max_tokens is deprecated, please use num_tokens instead.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "pickup_latitude (InputLayer)    [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "pickup_longitude (InputLayer)   [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropoff_latitude (InputLayer)   [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropoff_longitude (InputLayer)  [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "discretization (Discretization) (None, 1)            21          pickup_latitude[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "discretization_1 (Discretizatio (None, 1)            21          pickup_longitude[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "discretization_2 (Discretizatio (None, 1)            21          dropoff_latitude[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "discretization_3 (Discretizatio (None, 1)            21          dropoff_longitude[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "trip_start_day (InputLayer)     [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "trip_start_hour (InputLayer)    [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_3 (CategoryEn (None, 21)           0           discretization[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_4 (CategoryEn (None, 21)           0           discretization_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_5 (CategoryEn (None, 21)           0           discretization_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_6 (CategoryEn (None, 21)           0           discretization_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup (StringLookup)    (None, 1)            0           trip_start_day[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_1 (StringLookup)  (None, 1)            0           trip_start_hour[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_crossing_1 (CategoryCr (None, None)         0           category_encoding_3[0][0]        \n",
            "                                                                 category_encoding_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "category_crossing_2 (CategoryCr (None, None)         0           category_encoding_5[0][0]        \n",
            "                                                                 category_encoding_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding (CategoryEnco (None, 8)            0           string_lookup[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_1 (CategoryEn (None, 24)           0           string_lookup_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "hashing_1 (Hashing)             (None, None)         0           category_crossing_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "hashing_2 (Hashing)             (None, None)         0           category_crossing_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "trip_start_month (InputLayer)   [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "category_crossing (CategoryCros (None, None)         0           category_encoding[0][0]          \n",
            "                                                                 category_encoding_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 4)      1764        hashing_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 4)      1764        hashing_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "string_lookup_2 (StringLookup)  (None, 1)            0           trip_start_month[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "hashing (Hashing)               (None, None)         0           category_crossing[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "pickup_latitude_scaled (InputLa [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "pickup_longitude_scaled (InputL [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "distance (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum (TFOpLambda) (None, 4)            0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_1 (TFOpLambd (None, 4)            0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.strings.to_number (TFOpLambd (None, 1)            0           trip_start_day[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.strings.to_number_1 (TFOpLam (None, 1)            0           trip_start_hour[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.strings.to_number_2 (TFOpLam (None, 1)            0           trip_start_month[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_2 (CategoryEn (None, 12)           0           string_lookup_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "category_encoding_7 (CategoryEn (None, 192)          0           hashing[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18)           0           pickup_latitude[0][0]            \n",
            "                                                                 pickup_longitude[0][0]           \n",
            "                                                                 dropoff_latitude[0][0]           \n",
            "                                                                 dropoff_longitude[0][0]          \n",
            "                                                                 pickup_latitude_scaled[0][0]     \n",
            "                                                                 pickup_longitude_scaled[0][0]    \n",
            "                                                                 distance[0][0]                   \n",
            "                                                                 tf.math.reduce_sum[0][0]         \n",
            "                                                                 tf.math.reduce_sum_1[0][0]       \n",
            "                                                                 tf.strings.to_number[0][0]       \n",
            "                                                                 tf.strings.to_number_1[0][0]     \n",
            "                                                                 tf.strings.to_number_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 288)          0           category_encoding_2[0][0]        \n",
            "                                                                 category_encoding_7[0][0]        \n",
            "                                                                 category_encoding_3[0][0]        \n",
            "                                                                 category_encoding_4[0][0]        \n",
            "                                                                 category_encoding_5[0][0]        \n",
            "                                                                 category_encoding_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           608         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           9248        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32)           128         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64)           0           dense_1[0][0]                    \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           2080        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32)           128         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            33          batch_normalization_1[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 15,837\n",
            "Trainable params: 15,625\n",
            "Non-trainable params: 212\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "2021-08-24 16:14:14.087038: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
            "2021-08-24 16:14:14.087105: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
            "2021-08-24 16:14:14.098634: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "  1/100 [..............................] - ETA: 4:24 - loss: 194.4130 - rmse: 13.94322021-08-24 16:14:16.934658: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
            "2021-08-24 16:14:16.934728: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
            "  2/100 [..............................] - ETA: 23s - loss: 195.7764 - rmse: 13.9920 2021-08-24 16:14:17.065385: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
            "2021-08-24 16:14:17.118442: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
            "2021-08-24 16:14:17.172525: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/logs/train/plugins/profile/2021_08_24_16_14_17\n",
            "2021-08-24 16:14:17.204360: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.trace.json.gz\n",
            "2021-08-24 16:14:17.221349: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /content/logs/train/plugins/profile/2021_08_24_16_14_17\n",
            "2021-08-24 16:14:17.221558: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.memory_profile.json.gz\n",
            "2021-08-24 16:14:17.222375: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /content/logs/train/plugins/profile/2021_08_24_16_14_17Dumped tool data for xplane.pb to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.xplane.pb\n",
            "Dumped tool data for overview_page.pb to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.overview_page.pb\n",
            "Dumped tool data for input_pipeline.pb to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.input_pipeline.pb\n",
            "Dumped tool data for tensorflow_stats.pb to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.tensorflow_stats.pb\n",
            "Dumped tool data for kernel_stats.pb to /content/logs/train/plugins/profile/2021_08_24_16_14_17/0af8948a189e.kernel_stats.pb\n",
            "\n",
            "100/100 [==============================] - ETA: 0s - loss: 178.4072 - rmse: 13.3569WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "100/100 [==============================] - 18s 155ms/step - loss: 178.4072 - rmse: 13.3569 - val_loss: 5288.3711 - val_rmse: 72.7208\n",
            "Epoch 2/3\n",
            "100/100 [==============================] - 15s 146ms/step - loss: 122.3583 - rmse: 11.0616 - val_loss: 14565.4512 - val_rmse: 120.6873\n",
            "Epoch 3/3\n",
            "100/100 [==============================] - 14s 139ms/step - loss: 75.9046 - rmse: 8.7123 - val_loss: 8119.4277 - val_rmse: 90.1078\n",
            "Final Val RMSE:  72.72076416015625\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqzjsiKUGIP7"
      },
      "source": [
        "### **Vertex Tensorboard logging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX5nNH6H8euF",
        "outputId": "a8d1e5cf-71c3-45f0-c620-9235a6813883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash \n",
        "gcloud config set project us-gcp-ame-con-06f-npd-1\n",
        "\n",
        "# tb-gcp-uploader --help\n",
        "\n",
        "tb-gcp-uploader --tensorboard_resource_name \"projects/382989511509/locations/us-central1/tensorboards/4093385033186803712\" \\\n",
        "  --logdir=gs://rafiqhasandttl_01/ml_tensorlogs/ \\\n",
        "  --experiment_name=h1 --one_shot=True"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "INFO:tensorboard:Creating experiment\n",
            "View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+382989511509+locations+us-central1+tensorboards+4093385033186803712+experiments+h1\n",
            "INFO:tensorboard:Starting an upload cycle\n",
            "INFO:tensorboard:Starting logdir traversal of gs://rafiqhasandttl_01/ml_tensorlogs/\n",
            "INFO:tensorboard:GetLogdirSubdirectories: Starting to list directories via glob-ing.\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 0\n",
            "INFO:tensorboard:GlobAndListFiles: 2 files glob-ed at level 0\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 1\n",
            "INFO:tensorboard:GlobAndListFiles: 4 files glob-ed at level 1\n",
            "INFO:tensorboard:- Adding run for relative directory train\n",
            "INFO:tensorboard:- Adding run for relative directory validation\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 2\n",
            "INFO:tensorboard:GlobAndListFiles: 1 files glob-ed at level 2\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 3\n",
            "INFO:tensorboard:GlobAndListFiles: 1 files glob-ed at level 3\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 4\n",
            "INFO:tensorboard:GlobAndListFiles: 7 files glob-ed at level 4\n",
            "INFO:tensorboard:GlobAndListFiles: Starting to glob level 5\n",
            "INFO:tensorboard:GlobAndListFiles: 0 files glob-ed at level 5\n",
            "INFO:tensorboard:Ending logdir traversal of gs://rafiqhasandttl_01/ml_tensorlogs/\n",
            "INFO:tensorboard:Logdir sync took 1.588 seconds\n",
            "INFO:tensorboard:Creating event loading generators for 2 runs\n",
            "\u001b[1m[2021-08-24T16:45:48]\u001b[0m Started scanning logdir.\n",
            "\u001b[2K\u001b[32mData upload starting...\u001b[0m\rINFO:tensorboard:Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/train/events.out.tfevents.1629821654.0af8948a189e.874.3675.v2\n",
            "INFO:tensorboard:Skipping time series ('train', 'keras') with unsupported plugin name 'graph_keras_model'\n",
            "\u001b[2K\u001b[32mUploading binary object (235.4 kB)...\u001b[0m\rINFO:tensorboard:Trying request of 83 bytes\n",
            "INFO:tensorboard:Upload of (83 bytes) took 0.093 seconds\n",
            "INFO:tensorboard:Skipping time series ('train', 'batch_2') with unsupported plugin name 'graph_run_metadata_graph'\n",
            "INFO:tensorboard:Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/train/events.out.tfevents.1629821657.0af8948a189e.profile-empty\n",
            "INFO:tensorboard:Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/validation/events.out.tfevents.1629821668.0af8948a189e.874.6533.v2\n",
            "INFO:tensorboard:Trying request of 463 bytes\n",
            "\u001b[2K\u001b[32mUploading 9 scalars...\u001b[0m\rINFO:tensorboard:Upload of (463 bytes) took 0.089 seconds\n",
            "INFO:tensorboard:Trying request of 579 bytes\n",
            "\u001b[2K\u001b[32mUploading 12 scalars...\u001b[0m\rINFO:tensorboard:Upload of (579 bytes) took 0.106 seconds\n",
            "\u001b[1m[2021-08-24T16:45:50]\u001b[0m Total uploaded: 21 scalars, 0 tensors, 1 binary objects (235.4 kB)\n",
            "\u001b[2K\u001b[33mListening for new data in logdir...\u001b[0m\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "2021-08-24 16:45:45.260240: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "W0824 16:45:46.777312 139665146435456 _default.py:484] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
            "I0824 16:45:46.777934 139665146435456 uploader.py:262] Creating experiment\n",
            "I0824 16:45:46.943666 139665146435456 uploader.py:326] Starting an upload cycle\n",
            "I0824 16:45:46.944006 139665146435456 logdir_loader.py:65] Starting logdir traversal of gs://rafiqhasandttl_01/ml_tensorlogs/\n",
            "I0824 16:45:48.141810 139665146435456 io_wrapper.py:216] GetLogdirSubdirectories: Starting to list directories via glob-ing.\n",
            "I0824 16:45:48.142232 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 0\n",
            "I0824 16:45:48.209699 139665146435456 io_wrapper.py:136] GlobAndListFiles: 2 files glob-ed at level 0\n",
            "I0824 16:45:48.210104 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 1\n",
            "I0824 16:45:48.272962 139665146435456 io_wrapper.py:136] GlobAndListFiles: 4 files glob-ed at level 1\n",
            "I0824 16:45:48.273374 139665146435456 logdir_loader.py:71] - Adding run for relative directory train\n",
            "I0824 16:45:48.273540 139665146435456 logdir_loader.py:71] - Adding run for relative directory validation\n",
            "I0824 16:45:48.273641 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 2\n",
            "I0824 16:45:48.334336 139665146435456 io_wrapper.py:136] GlobAndListFiles: 1 files glob-ed at level 2\n",
            "I0824 16:45:48.334669 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 3\n",
            "I0824 16:45:48.403647 139665146435456 io_wrapper.py:136] GlobAndListFiles: 1 files glob-ed at level 3\n",
            "I0824 16:45:48.403983 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 4\n",
            "I0824 16:45:48.469073 139665146435456 io_wrapper.py:136] GlobAndListFiles: 7 files glob-ed at level 4\n",
            "I0824 16:45:48.469400 139665146435456 io_wrapper.py:133] GlobAndListFiles: Starting to glob level 5\n",
            "I0824 16:45:48.531114 139665146435456 io_wrapper.py:136] GlobAndListFiles: 0 files glob-ed at level 5\n",
            "I0824 16:45:48.531465 139665146435456 logdir_loader.py:80] Ending logdir traversal of gs://rafiqhasandttl_01/ml_tensorlogs/\n",
            "I0824 16:45:48.531577 139665146435456 uploader.py:331] Logdir sync took 1.588 seconds\n",
            "I0824 16:45:48.531924 139665146435456 logdir_loader.py:94] Creating event loading generators for 2 runs\n",
            "I0824 16:45:48.660014 139665146435456 directory_loader.py:128] Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/train/events.out.tfevents.1629821654.0af8948a189e.874.3675.v2\n",
            "I0824 16:45:48.886931 139665146435456 uploader.py:490] Skipping time series ('train', 'keras') with unsupported plugin name 'graph_keras_model'\n",
            "I0824 16:45:49.324270 139665146435456 uploader.py:1358] Trying request of 83 bytes\n",
            "I0824 16:45:49.417594 139665146435456 uploader.py:1362] Upload of (83 bytes) took 0.093 seconds\n",
            "I0824 16:45:49.418212 139665146435456 uploader.py:490] Skipping time series ('train', 'batch_2') with unsupported plugin name 'graph_run_metadata_graph'\n",
            "I0824 16:45:49.722947 139665146435456 directory_loader.py:128] Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/train/events.out.tfevents.1629821657.0af8948a189e.profile-empty\n",
            "I0824 16:45:49.981076 139665146435456 directory_loader.py:128] Loading data from path gs://rafiqhasandttl_01/ml_tensorlogs/validation/events.out.tfevents.1629821668.0af8948a189e.874.6533.v2\n",
            "I0824 16:45:50.600119 139665146435456 uploader.py:1358] Trying request of 463 bytes\n",
            "I0824 16:45:50.688661 139665146435456 uploader.py:1362] Upload of (463 bytes) took 0.089 seconds\n",
            "I0824 16:45:50.689302 139665146435456 uploader.py:1358] Trying request of 579 bytes\n",
            "I0824 16:45:50.795332 139665146435456 uploader.py:1362] Upload of (579 bytes) took 0.106 seconds\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbyvqXczGMxR"
      },
      "source": [
        "### **Packaging Trainer Script**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcclHxP0ON4",
        "outputId": "e1cf0502-6c36-43bd-b33b-1cbb7cdb453d"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "from setuptools import find_packages\n",
        "from setuptools import setup\n",
        "\n",
        "REQUIRED_PACKAGES = ['absl-py','folium', 'pandas', 'tensorflow_transform',\n",
        "                     'google-cloud','google-cloud-storage','google-cloud-firestore','google-api-python-client', 'google-auth']\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=find_packages(),\n",
        "    include_package_data=True,\n",
        "    description='Hasan - Vertex AI Taxi Trainer Job'\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAwM-Vla37QY",
        "outputId": "db3bb512-b597-4b37-fdf8-70465e80dbb8"
      },
      "source": [
        "%%writefile __init__.py\n",
        " "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing __init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C-77ZmU0jx0",
        "outputId": "88a6736c-2f6a-4496-e8b7-2014f2cdb7f8"
      },
      "source": [
        "##Create trainer package\n",
        "!rm -rf /content/trainer\n",
        "!rm -rf /content/dist/\n",
        "!rm -rf /content/trainer*\n",
        "\n",
        "!mkdir /content/trainer/\n",
        "!cp /content/task.py /content/trainer/\n",
        "!cp /content/__init__.py /content/trainer/\n",
        "!python /content/setup.py sdist"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\n",
            "running egg_info\n",
            "creating trainer.egg-info\n",
            "writing trainer.egg-info/PKG-INFO\n",
            "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
            "writing requirements to trainer.egg-info/requires.txt\n",
            "writing top-level names to trainer.egg-info/top_level.txt\n",
            "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
            "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
            "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating trainer-0.1\n",
            "creating trainer-0.1/trainer\n",
            "creating trainer-0.1/trainer.egg-info\n",
            "copying files to trainer-0.1...\n",
            "copying setup.py -> trainer-0.1\n",
            "copying trainer/__init__.py -> trainer-0.1/trainer\n",
            "copying trainer/task.py -> trainer-0.1/trainer\n",
            "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
            "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
            "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
            "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
            "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
            "Writing trainer-0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'trainer-0.1' (and everything under it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHqMtL3g4Lhh",
        "outputId": "f46d888a-47f8-4380-ece2-68a8f1490e40"
      },
      "source": [
        "# Copy trainer.gz to GCS training path\n",
        "!gsutil cp /content/dist/trainer-0.1.tar.gz gs://rafiqhasandttl_01/ml_scripts/taxi_dataset/trainer/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
            "Operation completed over 1 objects/7.2 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "volCF3HBAUi2"
      },
      "source": [
        "### **Creating KFP Vertex Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzM2tkT3Aa5r"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "from google.colab import auth\n",
        "from typing import NamedTuple\n",
        "\n",
        "import kfp\n",
        "# from google.cloud import aiplatform\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2 import compiler  # noqa: F811\n",
        "from kfp.v2.dsl import (ClassificationMetrics, Input, Metrics, Model, Output, Dataset,\n",
        "                        component)\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'us-gcp-ame-con-06f-npd-1'\n",
        "DISPLAY_NAME = 'taxi-custom-vertex-pipeline'\n",
        "REGION = 'us-central1'\n",
        "PIPELINE_ROOT = 'gs://rafiqhasandttl_01/ml_pipelines/'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFmgdY5UF5Hg"
      },
      "source": [
        "@component(\n",
        "    packages_to_install = [\n",
        "        \"tensorflow_transform\",\"tensorflow==2.5.0\"\n",
        "    ],\n",
        ")\n",
        "def run_tft_transform(\n",
        "    train_data_file: str,\n",
        "    transform_fn_output_dir: str,\n",
        "    metrics: Output[Metrics],\n",
        "    mlmd_dataset: Output[Dataset]\n",
        ") -> NamedTuple(\"Outputs\", [(\"fn_output_dir\", str)]): ##Output of COMPONENT\n",
        "    \n",
        "    import apache_beam as beam\n",
        "    import tensorflow_transform as tft\n",
        "    import tempfile\n",
        "    import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "    from tfx_bsl.public import tfxio\n",
        "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "    from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "    import tensorflow as tf\n",
        "    import tensorflow.keras as keras\n",
        "\n",
        "    CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
        "       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "       'dropoff_longitude']\n",
        "\n",
        "    RAW_DATA_FEATURE_SPEC = dict([\n",
        "        ('fare', tf.io.VarLenFeature(tf.float32)),\n",
        "        ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
        "        ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
        "        ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
        "        ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "        ])\n",
        "\n",
        "    # https://www.tensorflow.org/tfx/tutorials/transform/census\n",
        "    def tf_preprocessing_fn(inputs):\n",
        "        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "        pickup_latitude = inputs['pickup_latitude']\n",
        "        pickup_longitude = inputs['pickup_longitude']\n",
        "\n",
        "        #Calculate unique values of attribute\n",
        "        tft.vocabulary(inputs['trip_start_day'], vocab_filename='trip_start_day')\n",
        "\n",
        "        pickup_latitude_scaled = tft.scale_to_0_1(pickup_latitude)\n",
        "        pickup_longitude_scaled = tft.scale_to_0_1(pickup_longitude)\n",
        "        distance = ((inputs['pickup_latitude'] - inputs['dropoff_latitude'])**2 +  (inputs['pickup_longitude'] - inputs['dropoff_longitude'])**2)**0.5\n",
        "\n",
        "        return {\n",
        "            'pickup_latitude_scaled': pickup_latitude_scaled,\n",
        "            'pickup_longitude_scaled': pickup_longitude_scaled,\n",
        "            'distance': distance,\n",
        "            # 'dummy1': dummy1\n",
        "        }\n",
        "\n",
        "    def beam_run_transform_data(train_data_file, transform_fn_output_dir):\n",
        "      \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "      Read in the data using the CSV reader, and transform it using a\n",
        "      preprocessing pipeline that scales numeric data and converts categorical data\n",
        "      from strings to int64 values indices, by creating a vocabulary for each\n",
        "      category.\n",
        "\n",
        "      Args:\n",
        "        train_data_file: File containing training data\n",
        "        transform_fn_output_dir: Directory to write function\n",
        "      \"\"\"\n",
        "    \n",
        "      SCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "        tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)).schema\n",
        "\n",
        "      # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "      # of the block.\n",
        "      with beam.Pipeline() as pipeline:\n",
        "        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "          # Create a TFXIO to read the census data with the schema. To do this we\n",
        "          # need to list all columns in order since the schema doesn't specify the\n",
        "          # order of columns in the csv.\n",
        "          # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n",
        "          # accepts a PCollection[bytes] because we need to patch the records first\n",
        "          # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n",
        "          # to both read the CSV files and parse them to TFT inputs:\n",
        "          # csv_tfxio = tfxio.CsvTFXIO(...)\n",
        "          # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n",
        "          csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "              physical_format='text',\n",
        "              column_names=CSV_COLUMNS,\n",
        "              schema=SCHEMA)\n",
        "\n",
        "          # Read in raw data and convert using CSV TFXIO.  Note that we apply\n",
        "          # some Beam transformations here, which will not be encoded in the TF\n",
        "          # graph since we don't do the from within tf.Transform's methods\n",
        "          # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "          # to get data into a format that the CSV TFXIO can read, in particular\n",
        "          # removing spaces after commas.\n",
        "          raw_data = (\n",
        "              pipeline\n",
        "              | 'ReadTrainData' >> beam.io.ReadFromText(\n",
        "                  train_data_file, coder=beam.coders.BytesCoder())\n",
        "              | 'FixCommasTrainData' >> beam.Map(\n",
        "                  lambda line: line.replace(b', ', b','))\n",
        "              | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "          # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "          # the schema to read the CSV data, but we also need it to interpret\n",
        "          # raw_data.\n",
        "          raw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n",
        "\n",
        "          # The TFXIO output format is chosen for improved performance.\n",
        "          transform_fn = (\n",
        "              raw_dataset | tft_beam.AnalyzeDataset(tf_preprocessing_fn))\n",
        "\n",
        "          # Will write a SavedModel and metadata to transform_fn_output_dir, which can then\n",
        "          # be read by the tft.TFTransformOutput class.\n",
        "          _ = (\n",
        "              transform_fn\n",
        "              | 'WriteTransformFn' >> tft_beam.WriteTransformFn(transform_fn_output_dir))\n",
        "          \n",
        "    \n",
        "    #Main runner\n",
        "    beam_run_transform_data(train_data_file, transform_fn_output_dir)\n",
        "    metrics.log_metric(\"size\", \"dummy-1\")                               #Saving metric\n",
        "    mlmd_dataset.metadata[\"metadata\"] = \"value-1\"                        #Saving Metadata\n",
        "\n",
        "    return (transform_fn_output_dir, )"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUZnRTd0P6g"
      },
      "source": [
        "@kfp.dsl.pipeline(name=\"hasan-custom-vertex-pipeline-taxi\", pipeline_root=PIPELINE_ROOT)\n",
        "def pipeline(\n",
        "    display_name: str = DISPLAY_NAME,\n",
        "    project: str = PROJECT_ID,\n",
        "    gcp_region: str = REGION,\n",
        "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
        "    train_file: str = 'gs://rafiqhasandttl_01/taxi_dataset/train.csv', ##\"/content/train.csv\"\n",
        "    eval_file: str = 'gs://rafiqhasandttl_01/taxi_dataset/eval.csv', ##\"/content/eval.csv\"\n",
        "    transform_fn_output_dir: str = 'gs://rafiqhasandttl_01/ml_temporary/taxi_dataset/transform_fn',\n",
        "    model_save_location: str = 'gs://rafiqhasandttl_01/ml_models/taxi_dataset/',\n",
        "    epochs: int = 1\n",
        "):\n",
        "\n",
        "    import uuid\n",
        "    import json\n",
        "\n",
        "    # Determine CSV, label, and key columns\n",
        "    #Columns in training sheet -> Can have extra columns too\n",
        "    # CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
        "    #       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "    #       'dropoff_longitude']\n",
        "    # LABEL_COLUMN = 'fare'\n",
        "\n",
        "    # # Set default values for each CSV column( Including Y column )\n",
        "    # DEFAULTS = [[0.0], ['1'], ['1'],['1'],[0.0],[0.0],[0.0],[0.0]]\n",
        "\n",
        "    # bins_lat = [41.66367065, 41.85934972, 41.87740612, 41.87925508, 41.88099447,\n",
        "    #       41.88498719, 41.88530002, 41.89204214, 41.89207263, 41.89265811,\n",
        "    #       41.89830587, 41.89960211, 41.90026569, 41.90741282, 41.92187746,\n",
        "    #       41.92926299, 41.9442266 , 41.95402765, 41.97907082, 42.02122359]\n",
        "\n",
        "    # bins_lon = [-87.9136246 , -87.76550161, -87.68751552, -87.67161972,\n",
        "    #       -87.66341641, -87.65599818, -87.65253448, -87.64629348,\n",
        "    #       -87.642649  , -87.63784421, -87.63330804, -87.63274649,\n",
        "    #       -87.63186395, -87.62887416, -87.62621491, -87.62519214,\n",
        "    #       -87.62099291, -87.62076287, -87.61886836, -87.54093551]\n",
        "\n",
        "    # RAW_DATA_FEATURE_SPEC = dict([\n",
        "    #         ('fare', tf.io.VarLenFeature(tf.float32)),\n",
        "    #         ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
        "    #         ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
        "    #         ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
        "    #         ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "    #         ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "    #         ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "    #         ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
        "    #         ])\n",
        "\n",
        "    #STEP 1: Analyze Data using BEAM and create transformation fn\n",
        "    tft_op = run_tft_transform(\n",
        "                  train_data_file = train_file,\n",
        "                  transform_fn_output_dir= transform_fn_output_dir,\n",
        "              )\n",
        "\n",
        "    #STEP 2: For non Auto-ML call\n",
        "    training_args = ['--tranform_fn_dir', str(tft_op.outputs['fn_output_dir'])]\n",
        "\n",
        "    job_name = \"hasan-vertex-custom-\" + str(uuid.uuid4())\n",
        "    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
        "            project=PROJECT_ID,\n",
        "            display_name=job_name,\n",
        "            # model_display_name=\"hasan-vertex-taxi\",\n",
        "            python_package_gcs_uri=\"gs://rafiqhasandttl_01/ml_scripts/taxi_dataset/trainer/trainer-0.1.tar.gz\",\n",
        "            python_module=\"trainer.task\",\n",
        "            container_uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-5:latest',\n",
        "            staging_bucket='gs://rafiqhasandttl_01/ml_temporary/',\n",
        "            # base_output_dir=output_dir,\n",
        "            # model_serving_container_image_uri=model_serving_container_image_uri,\n",
        "            replica_count=1,\n",
        "            location=\"us-east4\",\n",
        "            machine_type=\"n1-standard-4\",\n",
        "            args=training_args)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr9p7Np4oTst"
      },
      "source": [
        "!rm -rf /content/*pipeline*.json"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvpActdOJHXz"
      },
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline, package_path=\"/content/hasan-custom-vertex-pipeline-taxi.json\"\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "c4NDwmZNJS0B",
        "outputId": "c311a909-625d-491f-b832-4191c39fa64c"
      },
      "source": [
        "from kfp.v2.google.client import AIPlatformClient\n",
        "\n",
        "api_client = AIPlatformClient(\n",
        "                project_id=PROJECT_ID,\n",
        "                region=REGION\n",
        "                )\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "    '/content/hasan-custom-vertex-pipeline-taxi.json',\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/kfp/v2/google/client/client.py:175: FutureWarning: AIPlatformClient will be deprecated in v1.9. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hasan-custom-vertex-pipeline-taxi-20210824152934?project=us-gcp-ame-con-06f-npd-1\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQh_Co8xL3iX"
      },
      "source": [
        "### **Local TMS Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpOhdxgwKyfE",
        "outputId": "84c1010c-d25d-451e-ec0e-eb85b77f7aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###Install TF Model server\n",
        "\n",
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!apt-get remove tensorflow-model-server\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update\n",
        "\n",
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package tensorflow-model-server\n",
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2943  100  2943    0     0  57705      0 --:--:-- --:--:-- --:--:-- 57705\n",
            "OK\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3,012 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [341 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [348 B]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,294 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,424 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,794 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,730 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [918 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,199 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]\n",
            "Fetched 12.8 MB in 4s (3,112 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tensorflow-model-server\n",
            "0 upgraded, 1 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 347 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.6.0 [347 MB]\n",
            "Fetched 347 MB in 6s (58.9 MB/s)\n",
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.6.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.6.0) ...\n",
            "Setting up tensorflow-model-server (2.6.0) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSIaeKHPKjuT",
        "outputId": "bf457f3e-e39c-46ed-d869-8c813ed86c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "#Create payload\n",
        "data_py = {\"instances\":[{'dropoff_latitude': 41.920452,\n",
        "                         'dropoff_longitude': -87.679955,\n",
        "                         'pickup_latitude': 41.952823,\n",
        "                         'pickup_longitude': -87.653244,\n",
        "                         'trip_start_day': \"1\",\n",
        "                         'trip_start_hour': \"5\",\n",
        "                         'trip_start_month': \"6\"}]}\n",
        "\n",
        "#Works as well\n",
        "# data_py = {\"inputs\":{'dropoff_latitude': [41.920452],\n",
        "#                          'dropoff_longitude': [-87.679955],\n",
        "#                          'pickup_latitude': [41.952823],\n",
        "#                          'pickup_longitude': [-87.653244],\n",
        "#                          'trip_start_day': [\"1\"],\n",
        "#                          'trip_start_hour': [\"5\"],\n",
        "#                          'trip_start_month': [\"6\"]}}\n",
        "\n",
        "data = json.dumps(data_py)\n",
        "print(\"payload: \", data)\n",
        "\n",
        "#Run request on TMS\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8505/v1/models/model:predict', data=data, headers=headers)\n",
        "json_response.text"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "payload:  {\"instances\": [{\"dropoff_latitude\": 41.920452, \"dropoff_longitude\": -87.679955, \"pickup_latitude\": 41.952823, \"pickup_longitude\": -87.653244, \"trip_start_day\": \"1\", \"trip_start_hour\": \"5\", \"trip_start_month\": \"6\"}]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n    \"predictions\": [[97.8500519]\\n    ]\\n}'"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-dBfncbLAAL",
        "outputId": "df3cbf83-51b1-4fb4-c121-ce1adeb49e88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###Start Tensorflow server\n",
        "# %%bash --bg \n",
        "# export TF_CPP_MIN_VLOG_LEVEL=0\n",
        "\n",
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8505 \\\n",
        "  --model_name=model \\\n",
        "  --model_base_path=\"gs://rafiqhasandttl_01/ml_models/taxi_dataset/\" >server.log 2>&1"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCgm5xJMLP9c",
        "outputId": "6265bba3-fb91-4ea1-cc5a-51fae66bde64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-24 17:19:54.426475: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1959423 microseconds.\n",
            "2021-08-24 17:19:54.553369: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at gs://rafiqhasandttl_01/ml_models/taxi_dataset/1/assets.extra/tf_serving_warmup_requests\n",
            "2021-08-24 17:19:55.193983: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
            "2021-08-24 17:19:55.196968: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
            "2021-08-24 17:19:55.197053: I tensorflow_serving/model_servers/server.cc:133] Using InsecureServerCredentials\n",
            "2021-08-24 17:19:55.197069: I tensorflow_serving/model_servers/server.cc:383] Profiler service is enabled\n",
            "2021-08-24 17:19:55.197652: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n",
            "2021-08-24 17:19:55.198369: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8505 ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NVoEm5bMQr0"
      },
      "source": [
        "### **Vertex AIP model prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6d2L673Jt8G",
        "outputId": "75196316-4a72-4e5d-dabc-fa95f9e61613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile input.json\n",
        "\n",
        "{\n",
        "   \"instances\":[\n",
        "      {\n",
        "         \"dropoff_latitude\":41.920452,\n",
        "         \"dropoff_longitude\":-87.679955,\n",
        "         \"pickup_latitude\":41.952823,\n",
        "         \"pickup_longitude\":-87.653244,\n",
        "         \"trip_start_day\":\"1\",\n",
        "         \"trip_start_hour\":\"5\",\n",
        "         \"trip_start_month\":\"6\"\n",
        "      }\n",
        "   ]\n",
        "}"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting input.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx5WK3kOJXHX",
        "outputId": "ecc15055-0888-4f63-e8ec-a4271c6b58d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash\n",
        "ENDPOINT_ID=\"6416785044072824832\"\n",
        "PROJECT_ID=\"us-gcp-ame-con-06f-npd-1\"\n",
        "INPUT_DATA_FILE=\"input.json\"\n",
        "\n",
        "curl \\\n",
        "-X POST \\\n",
        "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "https://us-east4-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-east4/endpoints/${ENDPOINT_ID}:predict \\\n",
        "-d \"@${INPUT_DATA_FILE}\""
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"predictions\": [\n",
            "    [\n",
            "      97.8500519\n",
            "    ]\n",
            "  ],\n",
            "  \"deployedModelId\": \"7539940569892519936\"\n",
            "}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   382    0   100  100   282    305    862 --:--:-- --:--:-- --:--:--  1168\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trwlnznJXSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}