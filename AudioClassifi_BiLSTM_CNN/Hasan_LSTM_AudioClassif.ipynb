{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Hasan Audio classification code\n",
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "#import matplotlib as plt\n",
    "import sys\n",
    "import io\n",
    "import glob\n",
    "import IPython\n",
    "import wave\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Input, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "load_feats_again = \"y\"\n",
    "max_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graph_spectrogram(wav_file):\n",
    "#     sound_info, frame_rate = get_wav_info(wav_file)\n",
    "#     pylab.figure(num=None, figsize=(19, 12))\n",
    "#     pylab.subplot(111)\n",
    "#     pylab.title('spectrogram of %r' % wav_file)\n",
    "#     pylab.specgram(sound_info, Fs=frame_rate)\n",
    "#     pylab.savefig('spectrogram.png')\n",
    "#     pylab.show()\n",
    "#     return(sound_info, frame_rate)\n",
    "\n",
    "# Calculate and plot spectrogram for a wav audio file\n",
    "def graph_spectrogram(wav_file):\n",
    "    data, rate = get_wav_info(wav_file)\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000 # Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx\n",
    "\n",
    "# Load a wav file\n",
    "# def get_wav_info(wav_file):\n",
    "#     rate, data = wavfile.read(wav_file)\n",
    "#     return rate, data\n",
    "\n",
    "# Used to standardize volume of audio clip\n",
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)\n",
    "    \n",
    "def get_wav_info(wav_file):\n",
    "    wav = wave.open(wav_file, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    sound_info = plt.fromstring(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return sound_info, frame_rate\n",
    "\n",
    "def get_batch_spectrograms(batch):\n",
    "    out = []\n",
    "    for b in batch:\n",
    "        out.append(graph_spectrogram(b))\n",
    "    return out\n",
    "\n",
    "def pad_with_zeros(x_arr, max_time):\n",
    "    x_new_arr = []\n",
    "    for x in x_arr:\n",
    "        x_dumm = np.zeros((101, max_time))\n",
    "        x_dumm[:,0:x.shape[1]] = x\n",
    "        x_new_arr.append(x_dumm)\n",
    "    return x_new_arr\n",
    "\n",
    "def find_max_audio_len(x_arr):\n",
    "    max_time = 0\n",
    "    for x in x_arr:\n",
    "        if x.shape[1] > max_time:\n",
    "            max_time = x.shape[1]\n",
    "    return max_time\n",
    "\n",
    "def reshape_x(x_in, max_time):\n",
    "    #Create full storage\n",
    "    x_out = np.zeros((len(x_in), max_time, 101))\n",
    "    \n",
    "    #Copy each song's feature to matrix\n",
    "    for i, x in enumerate(x_in):\n",
    "        x = np.transpose(x)\n",
    "        x_out[i,:,:] = x\n",
    "    #x = np.reshape(x, (len(x), max_time, 101))\n",
    "    return x_out\n",
    "\n",
    "def process_audio_in_folder(mypath):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    print(\"Running feature extraction ...\")\n",
    "    folders = [f for f in listdir(mypath) if not isfile(join(mypath, f))]\n",
    "    for fol in folders:\n",
    "        folderpath = mypath + \"\\\\\" + fol\n",
    "        for fil in listdir(folderpath):\n",
    "            if isfile(join(folderpath, fil)):\n",
    "                file_path = join(folderpath, fil)\n",
    "                print(file_path)\n",
    "                audio_feat = graph_spectrogram(file_path)\n",
    "                #print(\"Label: \" + fol + \" File: \" + fil +\" Features size \" + str(audio_feat.shape ))\n",
    "                x_array.append(audio_feat)\n",
    "                y_array.append(fol)\n",
    "    \n",
    "    #Do One hot of Y labels and return numpy\n",
    "    pd_y = pd.DataFrame(y_array)\n",
    "    y_array = pd.get_dummies(pd_y, [0]).values\n",
    "    return(x_array, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\matplotlib\\axes\\_axes.py:7221: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 8818)\n",
      "(101, 8818)\n"
     ]
    }
   ],
   "source": [
    "#Dummy -> Get features of files\n",
    "feats = get_batch_spectrograms([os.getcwd() + \"\\\\anupam\\\\HipRock\\\\1.wav\", os.getcwd() + \"\\\\anupam\\\\HipRock\\\\2.wav\"])\n",
    "for f in feats:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature extraction ...\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-01-02-03.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-01-02-10.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-01-02-19.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-01-02-21.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\matplotlib\\axes\\_axes.py:7221: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-01-06.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-01-20.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-02-01.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-02-03.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-02-06.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Negative\\03-01-05-02-02-02-24.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-01-03.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-01-05.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-01-07.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-01-12.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-02-03.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-02-09.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-02-12.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-01-02-20.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-02-01-03.wav\n",
      "C:\\Users\\hrafiq\\Documents\\HRafiq\\hrafiq\\Machine_Deep_learning\\Deep learning\\Audio_Classification\\AudioSentimentRNN\\Sound-Data\\tr\\Positive\\03-01-03-02-02-01-19.wav\n",
      "Number of files: 20\n",
      "Max timing: 3061\n",
      "X:  (20, 3061, 101)\n",
      "Y:  (20, 2)\n"
     ]
    }
   ],
   "source": [
    "if load_feats_again == \"y\":\n",
    "    #Extract features of all audio files\n",
    "    #Main folder -> Sub folders 1, 2 ,3 ( class wise ) -> Files per folder\n",
    "    x_arr, y_arr = process_audio_in_folder(os.getcwd() + \"\\Sound-Data\\\\tr\")\n",
    "    print(\"Number of files:\", len(y_arr))\n",
    "    \n",
    "    #Loop at all features and find max length\n",
    "    max_time = find_max_audio_len(x_arr)\n",
    "    print(\"Max timing:\" , max_time)\n",
    "    \n",
    "    #Pad x_array with zeros till max_length\n",
    "    x_arr = pad_with_zeros(x_arr, max_time)\n",
    "    \n",
    "    #Reshape to matrices\n",
    "    x_arr = reshape_x(x_arr, max_time)\n",
    "    \n",
    "    #Data loaded and reshaped\n",
    "    print(\"X: \",x_arr.shape)\n",
    "    print(\"Y: \",y_arr.shape)\n",
    "    \n",
    "    #Save features as npy files\n",
    "    np.save(os.getcwd() + \"\\\\x_arr\", x_arr)\n",
    "    np.save(os.getcwd() + \"\\\\y_arr\", y_arr)\n",
    "else:\n",
    "    x_arr = np.load(os.getcwd() + \"\\\\x_arr.npy\")\n",
    "    y_arr = np.load(os.getcwd() + \"\\\\y_arr.npy\")\n",
    "    \n",
    "    max_time = x_arr.shape[2]\n",
    "    \n",
    "    #Data loaded and reshaped\n",
    "    print(\"X: \",x_arr.shape)\n",
    "    print(\"Y: \",y_arr.shape)\n",
    "    print(\"Number of files:\", y_arr.shape[0])\n",
    "    print(\"Max timing:\" , max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(input_shape, numb_labels):\n",
    "    #Start creating Tensorflow based Keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add input layer\n",
    "    #You don't need to include the batch size here\n",
    "    #input of LSTM has shape as ( Batch size, Time steps, ....... )\n",
    "    #Input_shape is the dimension of ( Timesteps, ...... )\n",
    "    X_input = Input(shape = input_shape)\n",
    "\n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    X = Conv1D(filters=128, kernel_size=15, strides=4)(X_input)# CONV1D\n",
    "    X = BatchNormalization()(X)               # Batch normalization\n",
    "    X = Activation('relu')(X)                 # ReLu activation\n",
    "    #X = Dropout(0.8)(X)                       # dropout (use 0.8)\n",
    "\n",
    "    #Add first LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = Bidirectional(LSTM(units=128,return_sequences = True))(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    \n",
    "    #Add second LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = LSTM(units=128,return_sequences = True)(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    #X = Dropout(0.5)(X)    \n",
    "\n",
    "    #Add third LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = LSTM(units=128,return_sequences = False)(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    #X = Dropout(0.5)(X)    \n",
    "\n",
    "    #Dense layer with 64 neurons\n",
    "    X = Dense(64, activation='relu')(X)    \n",
    "    \n",
    "    #Dense layer with 32 neurons\n",
    "    X = Dense(32, activation='relu')(X)        \n",
    "    \n",
    "    #Add final softmax layer with 5 possible outputs\n",
    "    X = Dense(numb_labels, activation='softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, None, 101)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         194048    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 256)         263168    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, None, 128)         197120    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 796,834\n",
      "Trainable params: 796,578\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize model and show summary\n",
    "k_model = create_keras_model((None,101), y_arr.shape[1]) #Very important -> Create a dynamic length LSTM network so, max_time replaced with None\n",
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "k_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6987 - categorical_accuracy: 0.4000\n",
      "Epoch 2/40\n",
      "20/20 [==============================] - 16s 813ms/step - loss: 0.6879 - categorical_accuracy: 0.7000\n",
      "Epoch 3/40\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6792 - categorical_accuracy: 0.8000\n",
      "Epoch 4/40\n",
      "20/20 [==============================] - 17s 858ms/step - loss: 0.6648 - categorical_accuracy: 0.8000\n",
      "Epoch 5/40\n",
      "20/20 [==============================] - 17s 853ms/step - loss: 0.6795 - categorical_accuracy: 0.6000\n",
      "Epoch 6/40\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.6616 - categorical_accuracy: 0.7000\n",
      "Epoch 7/40\n",
      "20/20 [==============================] - 17s 853ms/step - loss: 0.6172 - categorical_accuracy: 0.8000\n",
      "Epoch 8/40\n",
      "20/20 [==============================] - 18s 875ms/step - loss: 0.5825 - categorical_accuracy: 0.8000\n",
      "Epoch 9/40\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.6639 - categorical_accuracy: 0.6000\n",
      "Epoch 10/40\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.6283 - categorical_accuracy: 0.7000\n",
      "Epoch 11/40\n",
      "20/20 [==============================] - 18s 910ms/step - loss: 0.6306 - categorical_accuracy: 0.7000\n",
      "Epoch 12/40\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.5901 - categorical_accuracy: 0.7000\n",
      "Epoch 13/40\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.5526 - categorical_accuracy: 0.8000\n",
      "Epoch 14/40\n",
      "20/20 [==============================] - 20s 981ms/step - loss: 0.5060 - categorical_accuracy: 0.8000\n",
      "Epoch 15/40\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.5570 - categorical_accuracy: 0.8000\n",
      "Epoch 16/40\n",
      "20/20 [==============================] - 17s 871ms/step - loss: 0.5799 - categorical_accuracy: 0.7000\n",
      "Epoch 17/40\n",
      "20/20 [==============================] - 17s 844ms/step - loss: 0.5832 - categorical_accuracy: 0.7000\n",
      "Epoch 18/40\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.7303 - categorical_accuracy: 0.6000\n",
      "Epoch 19/40\n",
      "20/20 [==============================] - 18s 924ms/step - loss: 0.4636 - categorical_accuracy: 0.8000\n",
      "Epoch 20/40\n",
      "20/20 [==============================] - 17s 871ms/step - loss: 0.6613 - categorical_accuracy: 0.6000\n",
      "Epoch 21/40\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.5517 - categorical_accuracy: 0.7000\n",
      "Epoch 22/40\n",
      "20/20 [==============================] - 20s 1s/step - loss: 0.5641 - categorical_accuracy: 0.7000\n",
      "Epoch 23/40\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 0.4700 - categorical_accuracy: 0.8000\n",
      "Epoch 24/40\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6344 - categorical_accuracy: 0.6000\n",
      "Epoch 25/40\n",
      "20/20 [==============================] - 20s 977ms/step - loss: 0.6196 - categorical_accuracy: 0.7000\n",
      "Epoch 26/40\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.4995 - categorical_accuracy: 0.8000\n",
      "Epoch 27/40\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.6579 - categorical_accuracy: 0.6000\n",
      "Epoch 28/40\n",
      "20/20 [==============================] - 18s 890ms/step - loss: 0.3795 - categorical_accuracy: 0.9000\n",
      "Epoch 29/40\n",
      "20/20 [==============================] - 16s 814ms/step - loss: 0.5507 - categorical_accuracy: 0.7000\n",
      "Epoch 30/40\n",
      "20/20 [==============================] - 18s 904ms/step - loss: 0.5620 - categorical_accuracy: 0.7000\n",
      "Epoch 31/40\n",
      "20/20 [==============================] - 25s 1s/step - loss: 0.4947 - categorical_accuracy: 0.7000\n",
      "Epoch 32/40\n",
      "20/20 [==============================] - 18s 909ms/step - loss: 0.5351 - categorical_accuracy: 0.7000\n",
      "Epoch 33/40\n",
      "20/20 [==============================] - 20s 1s/step - loss: 0.5700 - categorical_accuracy: 0.6000\n",
      "Epoch 34/40\n",
      "20/20 [==============================] - 20s 1s/step - loss: 0.6167 - categorical_accuracy: 0.7000\n",
      "Epoch 35/40\n",
      "20/20 [==============================] - 19s 958ms/step - loss: 0.4794 - categorical_accuracy: 0.8000\n",
      "Epoch 36/40\n",
      "20/20 [==============================] - 18s 915ms/step - loss: 0.5347 - categorical_accuracy: 0.7000\n",
      "Epoch 37/40\n",
      "20/20 [==============================] - 19s 967ms/step - loss: 0.5267 - categorical_accuracy: 0.7000\n",
      "Epoch 38/40\n",
      "20/20 [==============================] - 17s 857ms/step - loss: 0.5711 - categorical_accuracy: 0.7000\n",
      "Epoch 39/40\n",
      "20/20 [==============================] - 18s 875ms/step - loss: 0.4995 - categorical_accuracy: 0.8000\n",
      "Epoch 40/40\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.4192 - categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a0103b1f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training process\n",
    "k_model.fit(x_arr, y_arr, epochs = 40, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
