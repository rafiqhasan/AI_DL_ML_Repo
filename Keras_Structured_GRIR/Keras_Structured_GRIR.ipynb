{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "# import talos\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Hyper tuning\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GRIR_GCP_Data.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WERKS</th>\n",
       "      <th>SCENARIO</th>\n",
       "      <th>KTOKK</th>\n",
       "      <th>VSTATU</th>\n",
       "      <th>VPATD</th>\n",
       "      <th>EKORG</th>\n",
       "      <th>EKGRP</th>\n",
       "      <th>TOTGRQTY</th>\n",
       "      <th>TOTIRQTY</th>\n",
       "      <th>NODLGR</th>\n",
       "      <th>NODLIR</th>\n",
       "      <th>DIFGRIRD</th>\n",
       "      <th>DIFGRIRV</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ML01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>-80</td>\n",
       "      <td>-38100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ML01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>-107</td>\n",
       "      <td>-41600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ML01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>-107</td>\n",
       "      <td>-27600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>-96</td>\n",
       "      <td>-13800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ML01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>-146</td>\n",
       "      <td>-73500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WERKS  SCENARIO  KTOKK  VSTATU  VPATD  EKORG EKGRP  TOTGRQTY  TOTIRQTY  \\\n",
       "0  ML01         3      1       1     30      1     A         0        80   \n",
       "1  ML01         3      1       1     30      1     A         0       107   \n",
       "2  ML01         3      1       1     30      1     A         0       107   \n",
       "3  ML01         3      1       1     30      1     A         0        96   \n",
       "4  ML01         3      1       1     30      1     A         0       146   \n",
       "\n",
       "   NODLGR  NODLIR  DIFGRIRD  DIFGRIRV  STATUS  \n",
       "0       0      90       -80    -38100       1  \n",
       "1       0     177      -107    -41600       0  \n",
       "2       0     152      -107    -27600       1  \n",
       "3       0      79       -96    -13800       1  \n",
       "4       0     192      -146    -73500       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8279 entries, 0 to 8278\n",
      "Data columns (total 14 columns):\n",
      "WERKS       8279 non-null object\n",
      "SCENARIO    8279 non-null int64\n",
      "KTOKK       8279 non-null int64\n",
      "VSTATU      8279 non-null int64\n",
      "VPATD       8279 non-null int64\n",
      "EKORG       8279 non-null int64\n",
      "EKGRP       8279 non-null object\n",
      "TOTGRQTY    8279 non-null int64\n",
      "TOTIRQTY    8279 non-null int64\n",
      "NODLGR      8279 non-null int64\n",
      "NODLIR      8279 non-null int64\n",
      "DIFGRIRD    8279 non-null int64\n",
      "DIFGRIRV    8279 non-null int64\n",
      "STATUS      8279 non-null int64\n",
      "dtypes: int64(12), object(2)\n",
      "memory usage: 905.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8279 entries, 0 to 8278\n",
      "Data columns (total 14 columns):\n",
      "WERKS       8279 non-null object\n",
      "SCENARIO    8279 non-null category\n",
      "KTOKK       8279 non-null category\n",
      "VSTATU      8279 non-null category\n",
      "VPATD       8279 non-null int64\n",
      "EKORG       8279 non-null category\n",
      "EKGRP       8279 non-null object\n",
      "TOTGRQTY    8279 non-null int64\n",
      "TOTIRQTY    8279 non-null int64\n",
      "NODLGR      8279 non-null int64\n",
      "NODLIR      8279 non-null int64\n",
      "DIFGRIRD    8279 non-null int64\n",
      "DIFGRIRV    8279 non-null int64\n",
      "STATUS      8279 non-null int64\n",
      "dtypes: category(4), int64(8), object(2)\n",
      "memory usage: 679.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Mark some columns as categorical\n",
    "for col_cat in ['SCENARIO','KTOKK','VSTATU','EKORG']:\n",
    "    df[col_cat] = df[col_cat].astype('category')\n",
    "    \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare train and output columns\n",
    "df_x = df.drop(['STATUS'],axis=1)\n",
    "df_y = df['STATUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIFGRIRD</th>\n",
       "      <th>DIFGRIRV</th>\n",
       "      <th>EKORG</th>\n",
       "      <th>KTOKK</th>\n",
       "      <th>NODLGR</th>\n",
       "      <th>NODLIR</th>\n",
       "      <th>Random</th>\n",
       "      <th>SCENARIO</th>\n",
       "      <th>TOTGRQTY</th>\n",
       "      <th>TOTIRQTY</th>\n",
       "      <th>VPATD</th>\n",
       "      <th>VSTATU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865186</td>\n",
       "      <td>0.850320</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997738</td>\n",
       "      <td>0.485958</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879783</td>\n",
       "      <td>0.194014</td>\n",
       "      <td>0.338620</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>0.996530</td>\n",
       "      <td>0.787057</td>\n",
       "      <td>0.830635</td>\n",
       "      <td>0.727003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.596619</td>\n",
       "      <td>0.107376</td>\n",
       "      <td>0.989171</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.614764</td>\n",
       "      <td>0.205466</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093186</td>\n",
       "      <td>0.209803</td>\n",
       "      <td>2</td>\n",
       "      <td>0.711002</td>\n",
       "      <td>0.312491</td>\n",
       "      <td>0.797120</td>\n",
       "      <td>0.665292</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.287316</td>\n",
       "      <td>0.358338</td>\n",
       "      <td>0.774537</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.038072</td>\n",
       "      <td>0.248484</td>\n",
       "      <td>3</td>\n",
       "      <td>0.709403</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.894614</td>\n",
       "      <td>0.999331</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061270</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>0.922054</td>\n",
       "      <td>0.959982</td>\n",
       "      <td>0.549862</td>\n",
       "      <td>0.205517</td>\n",
       "      <td>4</td>\n",
       "      <td>0.205517</td>\n",
       "      <td>0.544833</td>\n",
       "      <td>0.205517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.640502</td>\n",
       "      <td>0.501720</td>\n",
       "      <td>0.414625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776710</td>\n",
       "      <td>0.514906</td>\n",
       "      <td>5</td>\n",
       "      <td>0.975722</td>\n",
       "      <td>0.791521</td>\n",
       "      <td>0.587118</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>0.982422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.286878</td>\n",
       "      <td>0.179726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999825</td>\n",
       "      <td>0.957352</td>\n",
       "      <td>0.895561</td>\n",
       "      <td>6</td>\n",
       "      <td>0.881185</td>\n",
       "      <td>0.739157</td>\n",
       "      <td>0.430504</td>\n",
       "      <td>0.881883</td>\n",
       "      <td>0.998032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.922265</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.275618</td>\n",
       "      <td>0.980325</td>\n",
       "      <td>0.810987</td>\n",
       "      <td>0.996811</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.287610</td>\n",
       "      <td>0.698262</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.549601</td>\n",
       "      <td>0.666214</td>\n",
       "      <td>0.714640</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.941271</td>\n",
       "      <td>0.227629</td>\n",
       "      <td>8</td>\n",
       "      <td>0.825911</td>\n",
       "      <td>0.769461</td>\n",
       "      <td>0.882082</td>\n",
       "      <td>0.708470</td>\n",
       "      <td>0.998032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.499860</td>\n",
       "      <td>0.953603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109492</td>\n",
       "      <td>0.504285</td>\n",
       "      <td>0.447540</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869874</td>\n",
       "      <td>0.665397</td>\n",
       "      <td>0.987354</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DIFGRIRD  DIFGRIRV     EKORG     KTOKK    NODLGR    NODLIR  Random  \\\n",
       "0  0.865186  0.850320  0.996530  1.000000  0.997738  0.485958       0   \n",
       "1  0.987979  0.835343  0.996530  0.787057  0.830635  0.727003       1   \n",
       "2  0.614764  0.205466  0.999289  1.000000  0.093186  0.209803       2   \n",
       "3  0.287316  0.358338  0.774537  0.999998  0.038072  0.248484       3   \n",
       "4  0.061270  0.108989  0.922054  0.959982  0.549862  0.205517       4   \n",
       "5  0.640502  0.501720  0.414625  1.000000  0.776710  0.514906       5   \n",
       "6  0.286878  0.179726  1.000000  0.999825  0.957352  0.895561       6   \n",
       "7  0.922265  0.890104  0.275618  0.980325  0.810987  0.996811       7   \n",
       "8  0.549601  0.666214  0.714640  0.999998  0.941271  0.227629       8   \n",
       "9  0.499860  0.953603  1.000000  0.109492  0.504285  0.447540       9   \n",
       "\n",
       "   SCENARIO  TOTGRQTY  TOTIRQTY     VPATD    VSTATU  \n",
       "0  0.879783  0.194014  0.338620  0.990500  1.000000  \n",
       "1  0.999995  0.596619  0.107376  0.989171  1.000000  \n",
       "2  0.711002  0.312491  0.797120  0.665292  1.000000  \n",
       "3  0.709403  0.027086  0.894614  0.999331  1.000000  \n",
       "4  0.205517  0.544833  0.205517  1.000000  0.998587  \n",
       "5  0.975722  0.791521  0.587118  0.997557  0.982422  \n",
       "6  0.881185  0.739157  0.430504  0.881883  0.998032  \n",
       "7  0.999995  0.562305  0.287610  0.698262  0.999999  \n",
       "8  0.825911  0.769461  0.882082  0.708470  0.998032  \n",
       "9  1.000000  0.869874  0.665397  0.987354  1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split dataset -> Split 10 times and choose the one with best P values( Significance test )\n",
    "p_res = {}\n",
    "t_res = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, random_state=i,stratify=df_y)\n",
    "\n",
    "    #Run Significance Tests on both the distributions( Train and Test ) for all numerical attributes\n",
    "    p_res = {}\n",
    "    for c_ in X_train.columns:\n",
    "        if not X_train[c_].dtype == 'object':\n",
    "            try:\n",
    "                _, a = scipy.stats.ks_2samp(X_train[c_].values,X_test[c_].values)\n",
    "                #print('P-value for column {} is {}'.format(c_.upper(), a))\n",
    "                p_res['Random'] = i\n",
    "                p_res[c_] = a\n",
    "            except:\n",
    "                p_res['Random'] = i\n",
    "                p_res[c_] = 'Error'\n",
    "    t_res.append(p_res)\n",
    "\n",
    "p_df = pd.DataFrame(t_res)\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7451 entries, 692 to 315\n",
      "Data columns (total 13 columns):\n",
      "WERKS       7451 non-null object\n",
      "SCENARIO    7451 non-null category\n",
      "KTOKK       7451 non-null category\n",
      "VSTATU      7451 non-null category\n",
      "VPATD       7451 non-null int64\n",
      "EKORG       7451 non-null category\n",
      "EKGRP       7451 non-null object\n",
      "TOTGRQTY    7451 non-null int64\n",
      "TOTIRQTY    7451 non-null int64\n",
      "NODLGR      7451 non-null int64\n",
      "NODLIR      7451 non-null int64\n",
      "DIFGRIRD    7451 non-null int64\n",
      "DIFGRIRV    7451 non-null int64\n",
      "dtypes: category(4), int64(7), object(2)\n",
      "memory usage: 611.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Split Train and Validation\n",
    "\n",
    "#Use the best split value from above after manual inspection\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, random_state=8,stratify=df_y)\n",
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['NODLGR'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VPATD</th>\n",
       "      <th>TOTGRQTY</th>\n",
       "      <th>TOTIRQTY</th>\n",
       "      <th>NODLGR</th>\n",
       "      <th>NODLIR</th>\n",
       "      <th>DIFGRIRD</th>\n",
       "      <th>DIFGRIRV</th>\n",
       "      <th>grminusirbyvpatd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "      <td>7451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.506039</td>\n",
       "      <td>0.329276</td>\n",
       "      <td>0.470508</td>\n",
       "      <td>0.384943</td>\n",
       "      <td>0.370619</td>\n",
       "      <td>0.626838</td>\n",
       "      <td>0.509348</td>\n",
       "      <td>-0.578462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.405868</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>0.312830</td>\n",
       "      <td>0.306502</td>\n",
       "      <td>0.311038</td>\n",
       "      <td>0.231224</td>\n",
       "      <td>0.169547</td>\n",
       "      <td>1.439549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043388</td>\n",
       "      <td>0.525547</td>\n",
       "      <td>0.488085</td>\n",
       "      <td>-0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.722628</td>\n",
       "      <td>0.554560</td>\n",
       "      <td>-0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.770073</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             VPATD     TOTGRQTY     TOTIRQTY       NODLGR       NODLIR  \\\n",
       "count  7451.000000  7451.000000  7451.000000  7451.000000  7451.000000   \n",
       "mean      0.506039     0.329276     0.470508     0.384943     0.370619   \n",
       "std       0.405868     0.314870     0.312830     0.306502     0.311038   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.270000     0.000000     0.043388   \n",
       "50%       0.500000     0.275000     0.495000     0.417910     0.338843   \n",
       "75%       1.000000     0.605000     0.735000     0.641791     0.636364   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          DIFGRIRD     DIFGRIRV  grminusirbyvpatd  \n",
       "count  7451.000000  7451.000000       7451.000000  \n",
       "mean      0.626838     0.509348         -0.578462  \n",
       "std       0.231224     0.169547          1.439549  \n",
       "min       0.000000     0.000000         -6.666667  \n",
       "25%       0.525547     0.488085         -0.833333  \n",
       "50%       0.722628     0.554560         -0.033333  \n",
       "75%       0.770073     0.599851          0.200000  \n",
       "max       1.000000     1.000000          1.666667  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature engineering steps\n",
    "x_train['grminusirbyvpatd'] = ( x_train['TOTGRQTY'] - x_train['TOTIRQTY'] ) / x_train['VPATD']\n",
    "x_test['grminusirbyvpatd'] = ( x_test['TOTGRQTY'] - x_test['TOTIRQTY'] ) / x_test['VPATD']\n",
    "\n",
    "#min max scaling\n",
    "for c_ in ['VPATD','TOTGRQTY','TOTIRQTY','NODLGR','NODLIR','DIFGRIRD','DIFGRIRV']:\n",
    "    mini = x_train[c_].min()\n",
    "    maxi = x_train[c_].max()\n",
    "    \n",
    "    x_train[c_] = (x_train[c_] - mini) / (maxi - mini)\n",
    "    x_test[c_] = (x_test[c_] - mini) / (maxi - mini)\n",
    "    \n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7451, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7451, 24)\n",
      "(828, 24)\n"
     ]
    }
   ],
   "source": [
    "#One hot encoders\n",
    "x_train = pd.get_dummies(x_train)\n",
    "x_test = pd.get_dummies(x_test)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "###Create keras Model( TF style )\n",
    "def create_train_model(x_train, y_train, x_test, y_test, params):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, input_dim=x_train.shape[1], activation='relu',kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    for l_ in range(params['hidden_layers']):\n",
    "        model.add(tf.keras.layers.Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1,   activation='sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \n",
    "                                       beta_2=params['beta_2'], epsilon=params['epsilon'], \n",
    "                                       decay=params['decay'])\n",
    "    model.compile(loss='binary_crossentropy',  optimizer=opt, metrics =['accuracy'])\n",
    "    \n",
    "    #Print Summary\n",
    "    model.summary()\n",
    "    \n",
    "    #Train\n",
    "    out = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=512, \n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)])\n",
    "    \n",
    "    #Accuracy test\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default params\n",
    "params_default = {\n",
    "    'lr' : 0.01,\n",
    "    'beta_1' : 0.99,\n",
    "    'beta_2' : 0.999,\n",
    "    'epsilon' : 1e-08,\n",
    "    'decay' : 0.01,\n",
    "    'hidden_layers' : 2\n",
    "}\n",
    "\n",
    "#hyper search params\n",
    "params_search = {\n",
    "    'lr' : [0.005, 0.05],\n",
    "    'beta_1' : [0.91, 0.93, 0.95, 0.97, 0.99],\n",
    "    'beta_2' : [0.9, 0.95, 0.97, 0.98, 0.99, 0.999],\n",
    "    'epsilon' : [1e-08],\n",
    "    'decay' : [0.007, 0.009, 0.01, 0.011, 0.013, 0.015, 0.02],\n",
    "    'hidden_layers' : [0, 1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 32)                800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,329\n",
      "Trainable params: 3,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Train on 7451 samples, validate on 828 samples\n",
      "Epoch 1/100\n",
      "7451/7451 [==============================] - ETA: 12s - loss: 0.8554 - acc: 0.49 - ETA: 0s - loss: 0.6283 - acc: 0.6546 - 1s 157us/step - loss: 0.6129 - acc: 0.6677 - val_loss: 0.6941 - val_acc: 0.6087\n",
      "Epoch 2/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.5295 - acc: 0.722 - ETA: 0s - loss: 0.5127 - acc: 0.734 - 0s 11us/step - loss: 0.5109 - acc: 0.7332 - val_loss: 0.6629 - val_acc: 0.6123\n",
      "Epoch 3/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.5030 - acc: 0.746 - ETA: 0s - loss: 0.4624 - acc: 0.761 - 0s 12us/step - loss: 0.4590 - acc: 0.7642 - val_loss: 0.6091 - val_acc: 0.6522\n",
      "Epoch 4/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.4339 - acc: 0.781 - ETA: 0s - loss: 0.4227 - acc: 0.790 - 0s 11us/step - loss: 0.4205 - acc: 0.7913 - val_loss: 0.5607 - val_acc: 0.7126\n",
      "Epoch 5/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.3850 - acc: 0.810 - ETA: 0s - loss: 0.3841 - acc: 0.815 - 0s 10us/step - loss: 0.3793 - acc: 0.8199 - val_loss: 0.5221 - val_acc: 0.7150\n",
      "Epoch 6/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.3366 - acc: 0.853 - ETA: 0s - loss: 0.3488 - acc: 0.834 - 0s 13us/step - loss: 0.3487 - acc: 0.8348 - val_loss: 0.4915 - val_acc: 0.7355\n",
      "Epoch 7/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.3307 - acc: 0.849 - ETA: 0s - loss: 0.3308 - acc: 0.837 - 0s 11us/step - loss: 0.3260 - acc: 0.8406 - val_loss: 0.4823 - val_acc: 0.7391\n",
      "Epoch 8/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2850 - acc: 0.884 - ETA: 0s - loss: 0.3041 - acc: 0.855 - 0s 11us/step - loss: 0.3040 - acc: 0.8565 - val_loss: 0.4474 - val_acc: 0.7633\n",
      "Epoch 9/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2955 - acc: 0.878 - ETA: 0s - loss: 0.2914 - acc: 0.865 - 0s 10us/step - loss: 0.2904 - acc: 0.8653 - val_loss: 0.4658 - val_acc: 0.7597\n",
      "Epoch 10/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2921 - acc: 0.869 - ETA: 0s - loss: 0.2742 - acc: 0.873 - 0s 11us/step - loss: 0.2747 - acc: 0.8769 - val_loss: 0.4413 - val_acc: 0.7814\n",
      "Epoch 11/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2601 - acc: 0.880 - ETA: 0s - loss: 0.2650 - acc: 0.878 - 0s 12us/step - loss: 0.2637 - acc: 0.8814 - val_loss: 0.4091 - val_acc: 0.7947\n",
      "Epoch 12/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2604 - acc: 0.878 - ETA: 0s - loss: 0.2542 - acc: 0.888 - 0s 9us/step - loss: 0.2546 - acc: 0.8879 - val_loss: 0.4284 - val_acc: 0.8043\n",
      "Epoch 13/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2454 - acc: 0.904 - ETA: 0s - loss: 0.2371 - acc: 0.895 - 0s 9us/step - loss: 0.2373 - acc: 0.8967 - val_loss: 0.4259 - val_acc: 0.8080\n",
      "Epoch 14/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.894 - ETA: 0s - loss: 0.2310 - acc: 0.902 - 0s 13us/step - loss: 0.2322 - acc: 0.9007 - val_loss: 0.4079 - val_acc: 0.8164\n",
      "Epoch 15/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2571 - acc: 0.871 - ETA: 0s - loss: 0.2337 - acc: 0.894 - 0s 12us/step - loss: 0.2302 - acc: 0.8984 - val_loss: 0.4074 - val_acc: 0.8285\n",
      "Epoch 16/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2003 - acc: 0.918 - ETA: 0s - loss: 0.2068 - acc: 0.911 - 0s 11us/step - loss: 0.2107 - acc: 0.9105 - val_loss: 0.4104 - val_acc: 0.8261\n",
      "Epoch 17/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2071 - acc: 0.906 - ETA: 0s - loss: 0.2052 - acc: 0.911 - 0s 11us/step - loss: 0.2073 - acc: 0.9109 - val_loss: 0.3890 - val_acc: 0.8333\n",
      "Epoch 18/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1975 - acc: 0.908 - ETA: 0s - loss: 0.2104 - acc: 0.912 - 0s 13us/step - loss: 0.2035 - acc: 0.9160 - val_loss: 0.4019 - val_acc: 0.8321\n",
      "Epoch 19/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1617 - acc: 0.925 - ETA: 0s - loss: 0.1961 - acc: 0.917 - 0s 10us/step - loss: 0.1971 - acc: 0.9176 - val_loss: 0.3747 - val_acc: 0.8466\n",
      "Epoch 20/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1854 - acc: 0.912 - ETA: 0s - loss: 0.1863 - acc: 0.920 - 0s 13us/step - loss: 0.1916 - acc: 0.9187 - val_loss: 0.3647 - val_acc: 0.8394\n",
      "Epoch 21/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2187 - acc: 0.904 - ETA: 0s - loss: 0.1836 - acc: 0.922 - 0s 10us/step - loss: 0.1846 - acc: 0.9214 - val_loss: 0.3578 - val_acc: 0.8430\n",
      "Epoch 22/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2041 - acc: 0.914 - ETA: 0s - loss: 0.1788 - acc: 0.923 - 0s 11us/step - loss: 0.1866 - acc: 0.9195 - val_loss: 0.3430 - val_acc: 0.8466\n",
      "Epoch 23/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1781 - acc: 0.933 - ETA: 0s - loss: 0.1750 - acc: 0.929 - 0s 15us/step - loss: 0.1780 - acc: 0.9265 - val_loss: 0.2938 - val_acc: 0.8780\n",
      "Epoch 24/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1574 - acc: 0.945 - ETA: 0s - loss: 0.1746 - acc: 0.928 - 0s 13us/step - loss: 0.1750 - acc: 0.9282 - val_loss: 0.2989 - val_acc: 0.8756\n",
      "Epoch 25/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.921 - ETA: 0s - loss: 0.1722 - acc: 0.927 - 0s 9us/step - loss: 0.1719 - acc: 0.9277 - val_loss: 0.2927 - val_acc: 0.8756\n",
      "Epoch 26/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1772 - acc: 0.933 - ETA: 0s - loss: 0.1684 - acc: 0.932 - 0s 11us/step - loss: 0.1705 - acc: 0.9307 - val_loss: 0.2575 - val_acc: 0.8865\n",
      "Epoch 27/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1477 - acc: 0.939 - ETA: 0s - loss: 0.1597 - acc: 0.935 - 0s 10us/step - loss: 0.1657 - acc: 0.9324 - val_loss: 0.2416 - val_acc: 0.8925\n",
      "Epoch 28/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1431 - acc: 0.947 - ETA: 0s - loss: 0.1700 - acc: 0.927 - 0s 9us/step - loss: 0.1700 - acc: 0.9274 - val_loss: 0.2359 - val_acc: 0.8841\n",
      "Epoch 29/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1799 - acc: 0.923 - ETA: 0s - loss: 0.1731 - acc: 0.924 - 0s 10us/step - loss: 0.1705 - acc: 0.9256 - val_loss: 0.2360 - val_acc: 0.8937\n",
      "Epoch 30/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.957 - ETA: 0s - loss: 0.1644 - acc: 0.930 - 0s 10us/step - loss: 0.1698 - acc: 0.9274 - val_loss: 0.2323 - val_acc: 0.8961\n",
      "Epoch 31/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1789 - acc: 0.929 - ETA: 0s - loss: 0.1643 - acc: 0.931 - 0s 10us/step - loss: 0.1643 - acc: 0.9318 - val_loss: 0.2186 - val_acc: 0.9046\n",
      "Epoch 32/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.925 - ETA: 0s - loss: 0.1585 - acc: 0.934 - 0s 11us/step - loss: 0.1559 - acc: 0.9363 - val_loss: 0.2024 - val_acc: 0.9094\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1686 - acc: 0.929 - ETA: 0s - loss: 0.1526 - acc: 0.936 - 0s 10us/step - loss: 0.1531 - acc: 0.9360 - val_loss: 0.2059 - val_acc: 0.9034\n",
      "Epoch 34/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1516 - acc: 0.931 - ETA: 0s - loss: 0.1512 - acc: 0.936 - 0s 12us/step - loss: 0.1502 - acc: 0.9368 - val_loss: 0.2132 - val_acc: 0.8986\n",
      "Epoch 35/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1835 - acc: 0.921 - ETA: 0s - loss: 0.1523 - acc: 0.936 - 0s 11us/step - loss: 0.1536 - acc: 0.9372 - val_loss: 0.2179 - val_acc: 0.9010\n",
      "Epoch 36/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1200 - acc: 0.947 - ETA: 0s - loss: 0.1422 - acc: 0.941 - 0s 13us/step - loss: 0.1487 - acc: 0.9376 - val_loss: 0.1983 - val_acc: 0.9058\n",
      "Epoch 37/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1405 - acc: 0.949 - ETA: 0s - loss: 0.1487 - acc: 0.937 - 0s 16us/step - loss: 0.1470 - acc: 0.9379 - val_loss: 0.1908 - val_acc: 0.9118\n",
      "Epoch 38/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1133 - acc: 0.955 - ETA: 0s - loss: 0.1483 - acc: 0.936 - ETA: 0s - loss: 0.1483 - acc: 0.938 - 0s 19us/step - loss: 0.1474 - acc: 0.9385 - val_loss: 0.1962 - val_acc: 0.9118\n",
      "Epoch 39/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1616 - acc: 0.939 - ETA: 0s - loss: 0.1455 - acc: 0.942 - 0s 14us/step - loss: 0.1463 - acc: 0.9392 - val_loss: 0.1923 - val_acc: 0.9118\n",
      "Epoch 40/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1519 - acc: 0.918 - ETA: 0s - loss: 0.1480 - acc: 0.935 - 0s 14us/step - loss: 0.1484 - acc: 0.9349 - val_loss: 0.1907 - val_acc: 0.9179\n",
      "Epoch 41/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1298 - acc: 0.957 - ETA: 0s - loss: 0.1379 - acc: 0.946 - 0s 14us/step - loss: 0.1351 - acc: 0.9454 - val_loss: 0.1875 - val_acc: 0.9094\n",
      "Epoch 42/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1308 - acc: 0.947 - ETA: 0s - loss: 0.1374 - acc: 0.944 - 0s 14us/step - loss: 0.1343 - acc: 0.9451 - val_loss: 0.1887 - val_acc: 0.9058\n",
      "Epoch 43/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1098 - acc: 0.955 - ETA: 0s - loss: 0.1279 - acc: 0.949 - 0s 15us/step - loss: 0.1364 - acc: 0.9442 - val_loss: 0.1810 - val_acc: 0.9167\n",
      "Epoch 44/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1107 - acc: 0.964 - ETA: 0s - loss: 0.1351 - acc: 0.944 - 0s 14us/step - loss: 0.1394 - acc: 0.9439 - val_loss: 0.1807 - val_acc: 0.9191\n",
      "Epoch 45/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1553 - acc: 0.923 - ETA: 0s - loss: 0.1294 - acc: 0.946 - 0s 13us/step - loss: 0.1336 - acc: 0.9439 - val_loss: 0.1889 - val_acc: 0.9167\n",
      "Epoch 46/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1444 - acc: 0.937 - ETA: 0s - loss: 0.1344 - acc: 0.944 - 0s 13us/step - loss: 0.1360 - acc: 0.9415 - val_loss: 0.1860 - val_acc: 0.9167\n",
      "Epoch 47/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1287 - acc: 0.943 - ETA: 0s - loss: 0.1321 - acc: 0.943 - 0s 12us/step - loss: 0.1322 - acc: 0.9427 - val_loss: 0.1785 - val_acc: 0.9203\n",
      "Epoch 48/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1042 - acc: 0.953 - ETA: 0s - loss: 0.1319 - acc: 0.944 - 0s 12us/step - loss: 0.1350 - acc: 0.9426 - val_loss: 0.1763 - val_acc: 0.9191\n",
      "Epoch 49/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1089 - acc: 0.951 - ETA: 0s - loss: 0.1323 - acc: 0.942 - 0s 15us/step - loss: 0.1303 - acc: 0.9462 - val_loss: 0.1750 - val_acc: 0.9203\n",
      "Epoch 50/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1334 - acc: 0.939 - ETA: 0s - loss: 0.1353 - acc: 0.941 - 0s 14us/step - loss: 0.1321 - acc: 0.9436 - val_loss: 0.1743 - val_acc: 0.9251\n",
      "Epoch 51/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1238 - acc: 0.957 - ETA: 0s - loss: 0.1289 - acc: 0.948 - 0s 14us/step - loss: 0.1299 - acc: 0.9465 - val_loss: 0.1715 - val_acc: 0.9239\n",
      "Epoch 52/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1168 - acc: 0.955 - ETA: 0s - loss: 0.1262 - acc: 0.946 - 0s 14us/step - loss: 0.1246 - acc: 0.9474 - val_loss: 0.1750 - val_acc: 0.9227\n",
      "Epoch 53/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1839 - acc: 0.910 - ETA: 0s - loss: 0.1295 - acc: 0.946 - 0s 11us/step - loss: 0.1348 - acc: 0.9440 - val_loss: 0.1792 - val_acc: 0.9203\n",
      "Epoch 54/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1268 - acc: 0.945 - ETA: 0s - loss: 0.1199 - acc: 0.951 - 0s 14us/step - loss: 0.1229 - acc: 0.9471 - val_loss: 0.1774 - val_acc: 0.9191\n",
      "Epoch 55/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0888 - acc: 0.966 - ETA: 0s - loss: 0.1203 - acc: 0.945 - 0s 15us/step - loss: 0.1232 - acc: 0.9446 - val_loss: 0.1720 - val_acc: 0.9203\n",
      "Epoch 56/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1223 - acc: 0.949 - ETA: 0s - loss: 0.1170 - acc: 0.949 - 0s 13us/step - loss: 0.1218 - acc: 0.9486 - val_loss: 0.1727 - val_acc: 0.9191\n",
      "Epoch 57/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1255 - acc: 0.949 - ETA: 0s - loss: 0.1213 - acc: 0.949 - 0s 11us/step - loss: 0.1254 - acc: 0.9487 - val_loss: 0.1721 - val_acc: 0.9215\n",
      "Epoch 58/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1018 - acc: 0.968 - ETA: 0s - loss: 0.1213 - acc: 0.950 - 0s 12us/step - loss: 0.1177 - acc: 0.9516 - val_loss: 0.1695 - val_acc: 0.9239\n",
      "Epoch 59/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1399 - acc: 0.937 - ETA: 0s - loss: 0.1237 - acc: 0.946 - 0s 12us/step - loss: 0.1242 - acc: 0.9452 - val_loss: 0.1665 - val_acc: 0.9275\n",
      "Epoch 60/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1146 - acc: 0.951 - ETA: 0s - loss: 0.1198 - acc: 0.951 - 0s 12us/step - loss: 0.1186 - acc: 0.9518 - val_loss: 0.1664 - val_acc: 0.9239\n",
      "Epoch 61/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0970 - acc: 0.960 - ETA: 0s - loss: 0.1266 - acc: 0.946 - 0s 12us/step - loss: 0.1239 - acc: 0.9482 - val_loss: 0.1661 - val_acc: 0.9227\n",
      "Epoch 62/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1250 - acc: 0.949 - ETA: 0s - loss: 0.1212 - acc: 0.949 - 0s 12us/step - loss: 0.1215 - acc: 0.9491 - val_loss: 0.1677 - val_acc: 0.9227\n",
      "Epoch 63/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1488 - acc: 0.927 - ETA: 0s - loss: 0.1253 - acc: 0.943 - 0s 14us/step - loss: 0.1235 - acc: 0.9460 - val_loss: 0.1650 - val_acc: 0.9251\n",
      "Epoch 64/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1087 - acc: 0.951 - ETA: 0s - loss: 0.1259 - acc: 0.945 - ETA: 0s - loss: 0.1286 - acc: 0.944 - 0s 20us/step - loss: 0.1298 - acc: 0.9435 - val_loss: 0.1643 - val_acc: 0.9239\n",
      "Epoch 65/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0973 - acc: 0.962 - ETA: 0s - loss: 0.1187 - acc: 0.952 - ETA: 0s - loss: 0.1139 - acc: 0.953 - 0s 25us/step - loss: 0.1122 - acc: 0.9542 - val_loss: 0.1647 - val_acc: 0.9239\n",
      "Epoch 66/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1432 - acc: 0.951 - ETA: 0s - loss: 0.1204 - acc: 0.949 - 0s 14us/step - loss: 0.1229 - acc: 0.9482 - val_loss: 0.1645 - val_acc: 0.9275\n",
      "Epoch 67/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1217 - acc: 0.945 - ETA: 0s - loss: 0.1233 - acc: 0.950 - ETA: 0s - loss: 0.1157 - acc: 0.954 - 0s 20us/step - loss: 0.1161 - acc: 0.9526 - val_loss: 0.1615 - val_acc: 0.9239\n",
      "Epoch 68/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1161 - acc: 0.949 - ETA: 0s - loss: 0.1097 - acc: 0.957 - ETA: 0s - loss: 0.1129 - acc: 0.955 - 0s 23us/step - loss: 0.1173 - acc: 0.9533 - val_loss: 0.1611 - val_acc: 0.9239\n",
      "Epoch 69/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1180 - acc: 0.951 - ETA: 0s - loss: 0.1077 - acc: 0.953 - ETA: 0s - loss: 0.1097 - acc: 0.952 - 0s 23us/step - loss: 0.1132 - acc: 0.9522 - val_loss: 0.1642 - val_acc: 0.9300\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1152 - acc: 0.953 - ETA: 0s - loss: 0.1189 - acc: 0.951 - ETA: 0s - loss: 0.1193 - acc: 0.948 - 0s 22us/step - loss: 0.1185 - acc: 0.9493 - val_loss: 0.1606 - val_acc: 0.9312\n",
      "Epoch 71/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0951 - acc: 0.962 - ETA: 0s - loss: 0.1104 - acc: 0.956 - ETA: 0s - loss: 0.1182 - acc: 0.952 - 0s 22us/step - loss: 0.1161 - acc: 0.9525 - val_loss: 0.1561 - val_acc: 0.9300\n",
      "Epoch 72/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1330 - acc: 0.943 - ETA: 0s - loss: 0.1168 - acc: 0.948 - ETA: 0s - loss: 0.1170 - acc: 0.949 - 0s 20us/step - loss: 0.1173 - acc: 0.9490 - val_loss: 0.1542 - val_acc: 0.9275\n",
      "Epoch 73/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1042 - acc: 0.959 - ETA: 0s - loss: 0.1100 - acc: 0.954 - 0s 12us/step - loss: 0.1121 - acc: 0.9534 - val_loss: 0.1559 - val_acc: 0.9287\n",
      "Epoch 74/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0862 - acc: 0.962 - ETA: 0s - loss: 0.1057 - acc: 0.956 - 0s 14us/step - loss: 0.1061 - acc: 0.9575 - val_loss: 0.1547 - val_acc: 0.9324\n",
      "Epoch 75/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0994 - acc: 0.955 - ETA: 0s - loss: 0.1041 - acc: 0.960 - ETA: 0s - loss: 0.1072 - acc: 0.957 - 0s 19us/step - loss: 0.1059 - acc: 0.9587 - val_loss: 0.1520 - val_acc: 0.9312\n",
      "Epoch 76/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1018 - acc: 0.966 - ETA: 0s - loss: 0.1047 - acc: 0.960 - 0s 17us/step - loss: 0.1076 - acc: 0.9577 - val_loss: 0.1510 - val_acc: 0.9300\n",
      "Epoch 77/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1377 - acc: 0.937 - ETA: 0s - loss: 0.1079 - acc: 0.953 - ETA: 0s - loss: 0.1219 - acc: 0.946 - 0s 20us/step - loss: 0.1193 - acc: 0.9478 - val_loss: 0.1511 - val_acc: 0.9324\n",
      "Epoch 78/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0985 - acc: 0.962 - ETA: 0s - loss: 0.1208 - acc: 0.950 - ETA: 0s - loss: 0.1123 - acc: 0.951 - 0s 18us/step - loss: 0.1124 - acc: 0.9511 - val_loss: 0.1515 - val_acc: 0.9300\n",
      "Epoch 79/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1176 - acc: 0.949 - ETA: 0s - loss: 0.1008 - acc: 0.957 - ETA: 0s - loss: 0.1088 - acc: 0.955 - 0s 19us/step - loss: 0.1115 - acc: 0.9534 - val_loss: 0.1529 - val_acc: 0.9312\n",
      "Epoch 80/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1405 - acc: 0.937 - ETA: 0s - loss: 0.1092 - acc: 0.955 - ETA: 0s - loss: 0.1052 - acc: 0.956 - 0s 19us/step - loss: 0.1057 - acc: 0.9565 - val_loss: 0.1520 - val_acc: 0.9312\n",
      "Epoch 81/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0914 - acc: 0.951 - ETA: 0s - loss: 0.1114 - acc: 0.953 - ETA: 0s - loss: 0.1124 - acc: 0.953 - 0s 17us/step - loss: 0.1189 - acc: 0.9509 - val_loss: 0.1505 - val_acc: 0.9312\n",
      "Epoch 82/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0851 - acc: 0.966 - ETA: 0s - loss: 0.1113 - acc: 0.953 - 0s 15us/step - loss: 0.1114 - acc: 0.9526 - val_loss: 0.1500 - val_acc: 0.9300\n",
      "Epoch 83/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1051 - acc: 0.955 - ETA: 0s - loss: 0.1019 - acc: 0.957 - 0s 15us/step - loss: 0.1032 - acc: 0.9576 - val_loss: 0.1491 - val_acc: 0.9287\n",
      "Epoch 84/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0935 - acc: 0.962 - ETA: 0s - loss: 0.1027 - acc: 0.957 - 0s 13us/step - loss: 0.1025 - acc: 0.9572 - val_loss: 0.1491 - val_acc: 0.9300\n",
      "Epoch 85/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1050 - acc: 0.951 - ETA: 0s - loss: 0.1130 - acc: 0.951 - 0s 14us/step - loss: 0.1061 - acc: 0.9540 - val_loss: 0.1493 - val_acc: 0.9287\n",
      "Epoch 86/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1215 - acc: 0.945 - ETA: 0s - loss: 0.0971 - acc: 0.961 - 0s 15us/step - loss: 0.1041 - acc: 0.9569 - val_loss: 0.1476 - val_acc: 0.9300\n",
      "Epoch 87/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1084 - acc: 0.953 - ETA: 0s - loss: 0.1039 - acc: 0.956 - 0s 13us/step - loss: 0.1053 - acc: 0.9565 - val_loss: 0.1470 - val_acc: 0.9324\n",
      "Epoch 88/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1116 - acc: 0.949 - ETA: 0s - loss: 0.1045 - acc: 0.954 - 0s 13us/step - loss: 0.1047 - acc: 0.9533 - val_loss: 0.1473 - val_acc: 0.9336\n",
      "Epoch 89/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0965 - acc: 0.947 - ETA: 0s - loss: 0.1052 - acc: 0.953 - 0s 13us/step - loss: 0.1017 - acc: 0.9560 - val_loss: 0.1518 - val_acc: 0.9275\n",
      "Epoch 90/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1004 - acc: 0.959 - ETA: 0s - loss: 0.0966 - acc: 0.959 - 0s 13us/step - loss: 0.1097 - acc: 0.9538 - val_loss: 0.1561 - val_acc: 0.9312\n",
      "Epoch 91/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1442 - acc: 0.935 - ETA: 0s - loss: 0.1101 - acc: 0.952 - 0s 13us/step - loss: 0.1136 - acc: 0.9494 - val_loss: 0.1476 - val_acc: 0.9336\n",
      "Epoch 92/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0945 - acc: 0.964 - ETA: 0s - loss: 0.1009 - acc: 0.959 - 0s 14us/step - loss: 0.1008 - acc: 0.9589 - val_loss: 0.1456 - val_acc: 0.9300\n",
      "Epoch 93/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1186 - acc: 0.945 - ETA: 0s - loss: 0.0995 - acc: 0.958 - 0s 13us/step - loss: 0.1030 - acc: 0.9568 - val_loss: 0.1455 - val_acc: 0.9312\n",
      "Epoch 94/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1170 - acc: 0.949 - ETA: 0s - loss: 0.1074 - acc: 0.956 - 0s 13us/step - loss: 0.1017 - acc: 0.9587 - val_loss: 0.1511 - val_acc: 0.9324\n",
      "Epoch 95/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1073 - acc: 0.960 - ETA: 0s - loss: 0.1004 - acc: 0.956 - 0s 14us/step - loss: 0.0979 - acc: 0.9580 - val_loss: 0.1530 - val_acc: 0.9396\n",
      "Epoch 96/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.1059 - acc: 0.947 - ETA: 0s - loss: 0.1012 - acc: 0.956 - 0s 15us/step - loss: 0.0995 - acc: 0.9579 - val_loss: 0.1473 - val_acc: 0.9348\n",
      "Epoch 97/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0636 - acc: 0.980 - ETA: 0s - loss: 0.0918 - acc: 0.965 - ETA: 0s - loss: 0.0965 - acc: 0.961 - 0s 21us/step - loss: 0.1000 - acc: 0.9604 - val_loss: 0.1445 - val_acc: 0.9348\n",
      "Epoch 98/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0711 - acc: 0.968 - ETA: 0s - loss: 0.0962 - acc: 0.958 - 0s 12us/step - loss: 0.1008 - acc: 0.9560 - val_loss: 0.1449 - val_acc: 0.9360\n",
      "Epoch 99/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0909 - acc: 0.962 - ETA: 0s - loss: 0.0980 - acc: 0.958 - 0s 11us/step - loss: 0.0961 - acc: 0.9601 - val_loss: 0.1457 - val_acc: 0.9384\n",
      "Epoch 100/100\n",
      "7451/7451 [==============================] - ETA: 0s - loss: 0.0925 - acc: 0.957 - ETA: 0s - loss: 0.0966 - acc: 0.958 - 0s 12us/step - loss: 0.0949 - acc: 0.9599 - val_loss: 0.1433 - val_acc: 0.9408\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmp6zhxc6mz\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_experimental_distribute': None, '_master': '', '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_train_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020D8E1BD5F8>, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_model_dir': 'C:\\\\Users\\\\hrafiq\\\\AppData\\\\Local\\\\Temp\\\\tmp6zhxc6mz', '_task_id': 0, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_eval_distribute': None, '_is_chief': True, '_service': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_device_fn': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x20d8cb07ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start training with default params\n",
    "o_ = create_train_model(x_train, y_train, x_test, y_test, params_default)\n",
    "\n",
    "#Convert to TF Estim\n",
    "tf.keras.estimator.model_to_estimator(\n",
    "    keras_model=o_['model']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
