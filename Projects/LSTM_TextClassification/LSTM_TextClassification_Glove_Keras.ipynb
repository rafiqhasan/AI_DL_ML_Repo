{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import keras as k\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Input, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GloVe embeddings in a matrix\n",
    "word2idx = {} # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "idx2word = {}\n",
    "weights = []\n",
    "\n",
    "#with (glove_data_directory / 'glove.6B.50d.txt').open('r') as file:\n",
    "with open('glove.6B/glove.6B.100d.txt', 'rb') as file:    \n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0].decode(\"utf-8\") # Word is first symbol on each line\n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "        word2idx[word] = index # PAD is our zeroth index so shift by one\n",
    "        idx2word[index] = word\n",
    "        weights.append(word_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, array([ 0.12804  ,  0.34131  ,  0.33106  , -0.026678 , -0.022675 ,\n",
      "       -1.0228   ,  0.65186  , -0.14204  ,  0.29102  ,  0.56137  ,\n",
      "       -0.1294   , -0.77794  , -0.014738 , -0.0082412,  0.19769  ,\n",
      "        0.42299  ,  0.64201  ,  0.89195  ,  0.28199  ,  0.038209 ,\n",
      "       -0.066105 , -0.39848  , -0.025111 ,  0.45934  , -0.45628  ,\n",
      "        0.36668  ,  0.56928  , -0.15604  , -0.82312  , -0.46751  ,\n",
      "        0.35949  ,  0.97564  , -0.047988 , -0.47062  ,  0.65927  ,\n",
      "        0.66212  ,  0.18403  , -0.052545 , -0.63723  , -0.53374  ,\n",
      "        0.50934  , -0.55863  ,  0.011983 ,  0.096682 ,  0.053548 ,\n",
      "        0.29566  , -0.15537  , -0.40615  , -0.58044  , -0.92148  ,\n",
      "        0.61701  , -0.019925 , -0.19368  ,  0.72811  ,  0.076774 ,\n",
      "       -1.6533   , -0.6374   , -0.060303 ,  1.9839   ,  0.13529  ,\n",
      "        0.47406  , -0.1415   , -0.37578  ,  0.15041  ,  0.89496  ,\n",
      "       -0.073249 ,  0.6373   , -0.33459  ,  0.97642  , -0.41846  ,\n",
      "        0.26385  ,  0.6476   , -0.057542 ,  0.0052852,  0.31263  ,\n",
      "       -0.0048151, -0.2146   , -0.18337  , -0.50711  , -0.10764  ,\n",
      "        0.15502  ,  0.12009  , -1.0232   ,  0.55192  , -1.4446   ,\n",
      "       -0.376    ,  0.11863  , -0.70984  , -0.16651  , -0.43381  ,\n",
      "        0.27606  ,  0.58443  , -0.3868   , -0.46112  ,  0.24229  ,\n",
      "       -0.21982  , -0.2088   , -0.37728  ,  1.2899   ,  0.13223  ],\n",
      "      dtype=float32))\n",
      "Sim1: [[0.7273129]]\n",
      "Sim2: [[0.81977755]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'funny'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function to get word embedding and index\n",
    "def get_embedding_and_index(v_word):\n",
    "    #Print embeddings for a word\n",
    "    ix = word2idx[v_word]\n",
    "    return(ix , weights[ix])\n",
    "\n",
    "print(get_embedding_and_index(\"product\"))\n",
    "emb1 = get_embedding_and_index(\"requirements\")\n",
    "emb2 = get_embedding_and_index(\"compliance\")\n",
    "emb3 = get_embedding_and_index(\"standards\")\n",
    "print(\"Sim1:\", cosine_similarity(emb1[1].reshape(1,-1), emb2[1].reshape(1,-1)))\n",
    "print(\"Sim2:\", cosine_similarity(emb1[1].reshape(1,-1), emb3[1].reshape(1,-1)))\n",
    "idx2word[5466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = emb_dim = 50\n",
    "vocab_len = len(word2idx) + 1\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of sentences to list of indices\n",
    "def sentences_to_indices(v_sentences,max_len):\n",
    "    #print(len(v_sentences))\n",
    "    sentence_idxs = np.zeros((len(v_sentences),max_len),np.int64)\n",
    "\n",
    "    #Pick up each sentence\n",
    "    for isx, v_s in enumerate(v_sentences):\n",
    "        #sentence to words\n",
    "        words = v_s.lower().split()\n",
    "\n",
    "        #For each word convert to index:\n",
    "        #print(isx, words)\n",
    "        for iwx, w in enumerate(words):\n",
    "            sentence_idxs[isx, iwx] = get_embedding_and_index(w)[0]\n",
    "\n",
    "    return(sentence_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5466, 73048,     0,     0,     0],\n",
       "       [ 8235,   282,  1444,     0,     0],\n",
       "       [  565,    14,  1188,    10,    81]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_to_indices([\"funny lol\", \"lets play baseball\", \"food is ready for you\"], max_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of sentences to list of embed vectors (m, max_len) -> (m, max_len, embed_size)\n",
    "def sentences_to_embeds(v_sentences, max_len):\n",
    "    m = len(v_sentences)\n",
    "    \n",
    "    #Convert sentences to indices first\n",
    "    sen_idx = sentences_to_indices(v_sentences, max_len = max_len)\n",
    "    \n",
    "    #Declare array of zeros first of shape (m, max_len, embed_size)\n",
    "    sen_embs = np.zeros((m, max_len, embed_size))\n",
    "    \n",
    "    #For each index of word, convert to embedding\n",
    "    for s_ix, s_txt in enumerate(v_sentences):\n",
    "        #print(s_txt)\n",
    "        for word_index in range(max_len):\n",
    "            #Replace 0s with word embeddings\n",
    "            #if s_ix == 0 and word_index == 0:\n",
    "                #print(weights[int(sen_idx[s_ix, word_index])])\n",
    "                #print(sen_idx[s_ix, word_index])\n",
    "            sen_embs[s_ix,word_index,:] = weights[int(sen_idx[s_ix, word_index])]\n",
    "    \n",
    "    #Return embeddings\n",
    "    return(sen_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.014546999707818031"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_to_embeds([\"funny lol\", \"lets play baseball\", \"food is ready for you\"], max_len = 10)[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text  label\n",
       "0           never talk to me again      3\n",
       "1  I am proud of your achievements      2\n",
       "2   It is the worst day in my life      3\n",
       "3                 Miss you so much      0\n",
       "4                     food is life      4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load training data\n",
    "all_data = pd.read_csv('train_data.csv')\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get length of maximum sentence\n",
    "max_len = all_data.text.map(lambda x: len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create one hot encoding of labels\n",
    "def create_yoh_list(v_classes, num_classes):\n",
    "    yoh_class = np.zeros((len(v_classes),num_classes))\n",
    "    \n",
    "    for ix, v_cl in enumerate(v_classes):\n",
    "        yoh_class[ix, v_cl] = 1\n",
    "    \n",
    "    return(yoh_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test one hot encoder code\n",
    "create_yoh_list([1,3,4,2],num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Def class to text\n",
    "def class_to_text(cls):\n",
    "    dict_class = {0:'love', 1:'playful', 2:'happy',3:'sad', 4:'foodie'}\n",
    "    return dict_class[cls]\n",
    "\n",
    "class_to_text(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 31 to 134\n",
      "Data columns (total 2 columns):\n",
      "text     179 non-null object\n",
      "label    179 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9 entries, 37 to 144\n",
      "Data columns (total 2 columns):\n",
      "text     9 non-null object\n",
      "label    9 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 216.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#Random split dataset into Train and Validation\n",
    "train_data = all_data.sample(frac=0.95,random_state=1)\n",
    "valid_data = all_data.drop(train_data.index)\n",
    "\n",
    "train_data.info()\n",
    "valid_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Keras' embedding layer\n",
    "#Emb layer => Takes word's index as input and converts it to word embeddings from preloaded GloVe embedding\n",
    "def get_embedding_layer():\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for index in range(len(weights)):\n",
    "        emb_matrix[index, :] = weights[index]\n",
    "\n",
    "    e = Embedding(len(word2idx) + 1, 50, input_length=max_len, trainable=False)\n",
    "    \n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    e.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix\n",
    "    e.set_weights([emb_matrix])\n",
    "    return(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.014547\n"
     ]
    }
   ],
   "source": [
    "#Test embedding layer\n",
    "embed_layer = get_embedding_layer()\n",
    "print(embed_layer.get_weights()[0][5466][0]) #Get embedding of second word's \"funny\" first feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(max_len, embed_layer):\n",
    "    #Start creating Tensorflow based Keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add input layer\n",
    "    #You don't need to include the batch size here\n",
    "    Inp = Input(shape=(max_len,))\n",
    "\n",
    "    #Call embedding layer and pass input\n",
    "    X = embed_layer(Inp)\n",
    "\n",
    "    #Add first LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = Bidirectional(LSTM(units=128,return_sequences = True))(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    #Add second LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = Bidirectional(LSTM(units=256,return_sequences = True))(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    X = Dropout(0.5)(X)    \n",
    "\n",
    "    #Add third LSTM layer\n",
    "    ###Very important -> Return_sequences is used to refer that every LSTM time step will return output or only last one\n",
    "    X = LSTM(units=128,return_sequences = False)(X)\n",
    "    \n",
    "    #Add dropout 0.5\n",
    "    X = Dropout(0.5)(X)    \n",
    "\n",
    "    #Add final softmax layer with 5 possible outputs\n",
    "    X = Dense(5, activation='softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=Inp, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 10, 256)           183296    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 10, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               328192    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 21,562,807\n",
      "Trainable params: 1,562,757\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize model and show summary\n",
    "k_model = create_keras_model(max_len ,embed_layer)\n",
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare training data using helper methods created above\n",
    "X_train = sentences_to_indices(train_data['text'].values, max_len = max_len)\n",
    "Y_train = create_yoh_list(train_data['label'].values,num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "179/179 [==============================] - 5s 26ms/step - loss: 1.6108 - acc: 0.2179\n",
      "Epoch 2/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 1.5220 - acc: 0.3184\n",
      "Epoch 3/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 1.5021 - acc: 0.3128\n",
      "Epoch 4/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 1.4189 - acc: 0.4022\n",
      "Epoch 5/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 1.2564 - acc: 0.4804\n",
      "Epoch 6/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 1.1491 - acc: 0.5028\n",
      "Epoch 7/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.9138 - acc: 0.6145\n",
      "Epoch 8/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.7764 - acc: 0.6983\n",
      "Epoch 9/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.5870 - acc: 0.7877\n",
      "Epoch 10/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.5592 - acc: 0.7877\n",
      "Epoch 11/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.5474 - acc: 0.8045\n",
      "Epoch 12/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.5008 - acc: 0.8492\n",
      "Epoch 13/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.3664 - acc: 0.8827\n",
      "Epoch 14/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.3438 - acc: 0.8771\n",
      "Epoch 15/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.3256 - acc: 0.8771\n",
      "Epoch 16/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.2746 - acc: 0.9050\n",
      "Epoch 17/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.3157 - acc: 0.8771\n",
      "Epoch 18/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.2556 - acc: 0.8939\n",
      "Epoch 19/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.2006 - acc: 0.9218\n",
      "Epoch 20/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.2581 - acc: 0.8994\n",
      "Epoch 21/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.2775 - acc: 0.9050\n",
      "Epoch 22/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.2235 - acc: 0.9385\n",
      "Epoch 23/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1515 - acc: 0.9553\n",
      "Epoch 24/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1778 - acc: 0.9385\n",
      "Epoch 25/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1731 - acc: 0.9497\n",
      "Epoch 26/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.4098 - acc: 0.8939\n",
      "Epoch 27/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.3645 - acc: 0.8771\n",
      "Epoch 28/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.1904 - acc: 0.9330\n",
      "Epoch 29/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1574 - acc: 0.9330\n",
      "Epoch 30/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1108 - acc: 0.9777\n",
      "Epoch 31/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0520 - acc: 0.9944\n",
      "Epoch 32/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0630 - acc: 0.9832\n",
      "Epoch 33/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0627 - acc: 0.9832\n",
      "Epoch 34/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0528 - acc: 0.9888\n",
      "Epoch 35/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1587 - acc: 0.9441\n",
      "Epoch 36/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1657 - acc: 0.9330\n",
      "Epoch 37/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1221 - acc: 0.9497\n",
      "Epoch 38/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0976 - acc: 0.9721\n",
      "Epoch 39/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1125 - acc: 0.9665\n",
      "Epoch 40/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0843 - acc: 0.9777\n",
      "Epoch 41/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0415 - acc: 0.9888\n",
      "Epoch 42/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0375 - acc: 0.9888\n",
      "Epoch 43/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0303 - acc: 0.9888\n",
      "Epoch 44/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0508 - acc: 0.9832\n",
      "Epoch 45/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0669 - acc: 0.9832\n",
      "Epoch 46/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1102 - acc: 0.9665\n",
      "Epoch 47/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1549 - acc: 0.9609\n",
      "Epoch 48/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0621 - acc: 0.9721\n",
      "Epoch 49/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0605 - acc: 0.9665\n",
      "Epoch 50/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0775 - acc: 0.9721\n",
      "Epoch 51/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0640 - acc: 0.9777\n",
      "Epoch 52/100\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0692 - acc: 0.9721\n",
      "Epoch 53/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.1722 - acc: 0.9330\n",
      "Epoch 54/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.2240 - acc: 0.9385\n",
      "Epoch 55/100\n",
      "179/179 [==============================] - 2s 10ms/step - loss: 0.1527 - acc: 0.9497\n",
      "Epoch 56/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0875 - acc: 0.9777\n",
      "Epoch 57/100\n",
      "179/179 [==============================] - 2s 10ms/step - loss: 0.0774 - acc: 0.9665\n",
      "Epoch 58/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0511 - acc: 0.9888\n",
      "Epoch 59/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0283 - acc: 0.9888\n",
      "Epoch 60/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0237 - acc: 0.9888\n",
      "Epoch 61/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.9888A: 0s - loss: 0.0246 - acc: 0.9\n",
      "Epoch 62/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0121 - acc: 0.9944\n",
      "Epoch 63/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0121 - acc: 0.9944\n",
      "Epoch 64/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0143 - acc: 0.9944\n",
      "Epoch 65/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0100 - acc: 0.9944\n",
      "Epoch 66/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0147 - acc: 0.9888\n",
      "Epoch 67/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0095 - acc: 0.9944\n",
      "Epoch 69/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0103 - acc: 0.9944\n",
      "Epoch 70/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0207 - acc: 0.9888\n",
      "Epoch 71/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0116 - acc: 0.9944\n",
      "Epoch 72/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0123 - acc: 0.9944\n",
      "Epoch 73/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0110 - acc: 0.9944\n",
      "Epoch 74/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0132 - acc: 0.9888\n",
      "Epoch 75/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0121 - acc: 0.9944\n",
      "Epoch 76/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0095 - acc: 0.9944\n",
      "Epoch 77/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0161 - acc: 0.9888\n",
      "Epoch 79/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0111 - acc: 0.9944\n",
      "Epoch 80/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0160 - acc: 0.9944\n",
      "Epoch 81/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0180 - acc: 0.9944\n",
      "Epoch 82/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0094 - acc: 0.9944\n",
      "Epoch 83/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0124 - acc: 0.9944\n",
      "Epoch 85/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.9888\n",
      "Epoch 86/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0091 - acc: 0.9944\n",
      "Epoch 87/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0077 - acc: 0.9944\n",
      "Epoch 88/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0120 - acc: 0.9944\n",
      "Epoch 89/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0091 - acc: 0.9944\n",
      "Epoch 91/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0110 - acc: 0.9944\n",
      "Epoch 92/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0092 - acc: 0.9944\n",
      "Epoch 96/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0080 - acc: 0.9944\n",
      "Epoch 97/100\n",
      "179/179 [==============================] - 2s 9ms/step - loss: 0.0187 - acc: 0.9944\n",
      "Epoch 98/100\n",
      "179/179 [==============================] - 1s 8ms/step - loss: 0.0111 - acc: 0.9944\n",
      "Epoch 99/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.9888\n",
      "Epoch 100/100\n",
      "179/179 [==============================] - 1s 7ms/step - loss: 0.0109 - acc: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201008d5d68>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training process\n",
    "k_model.fit(X_train, Y_train, epochs = 100, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 48ms/step\n",
      "Test accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "X_valid = sentences_to_indices(valid_data['text'].values, max_len = max_len)\n",
    "Y_valid = create_yoh_list(valid_data['label'].values,num_classes = 5)\n",
    "\n",
    "#Run on validation dataset\n",
    "loss, acc = k_model.evaluate(X_valid, Y_valid)\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop it ! --->  sad\n"
     ]
    }
   ],
   "source": [
    "#Test model with custom sentence\n",
    "test_sentence = 'stop it !'\n",
    "x_test = np.array([test_sentence])\n",
    "X_test_indices = sentences_to_indices(x_test, max_len = max_len)\n",
    "print(test_sentence +' --->  '+  class_to_text(int(np.argmax(k_model.predict(X_test_indices)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save trained model on system\n",
    "k_model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not happy --->  sad\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained model trial\n",
    "k_model_trained = load_model('trained_model.h5')\n",
    "\n",
    "test_sentence = 'i am not happy'\n",
    "x_test = np.array([test_sentence])\n",
    "X_test_indices = sentences_to_indices(x_test, max_len = max_len)\n",
    "k_model_trained.predict(X_test_indices)\n",
    "print(test_sentence +' --->  '+  class_to_text(int(np.argmax(k_model_trained.predict(X_test_indices)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
