{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c031cc-a286-4d30-945a-3459a24058c0",
   "metadata": {},
   "source": [
    "### Write file task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a987aafa-3a85-4b14-9c35-c74d6830ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=313e6726477b1117be4b12ad7df401812f7e7a5f55d047b8e94af1790dd3a20b\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b951188-d31a-4fa4-b6ec-14c3a98a935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "# Owner - Hasan Rafiq\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "  \n",
    "# Determine CSV, label, and key columns\n",
    "#Columns in training sheet -> Can have extra columns too\n",
    "CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
    "       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "       'dropoff_longitude']\n",
    "LABEL_COLUMN = 'fare'\n",
    "\n",
    "# Set default values for each CSV column( Including Y column )\n",
    "DEFAULTS = [[0.0], ['1'], ['1'],['1'],[0.0],[0.0],[0.0],[0.0]]\n",
    "\n",
    "bins_lat = [41.66367065, 41.85934972, 41.87740612, 41.87925508, 41.88099447,\n",
    "       41.88498719, 41.88530002, 41.89204214, 41.89207263, 41.89265811,\n",
    "       41.89830587, 41.89960211, 41.90026569, 41.90741282, 41.92187746,\n",
    "       41.92926299, 41.9442266 , 41.95402765, 41.97907082, 42.02122359]\n",
    "\n",
    "bins_lon = [-87.9136246 , -87.76550161, -87.68751552, -87.67161972,\n",
    "       -87.66341641, -87.65599818, -87.65253448, -87.64629348,\n",
    "       -87.642649  , -87.63784421, -87.63330804, -87.63274649,\n",
    "       -87.63186395, -87.62887416, -87.62621491, -87.62519214,\n",
    "       -87.62099291, -87.62076287, -87.61886836, -87.54093551]\n",
    "\n",
    "RAW_DATA_FEATURE_SPEC = dict([\n",
    "        ('fare', tf.io.VarLenFeature(tf.float32)),\n",
    "        ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
    "        ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ])\n",
    "    \n",
    "###############################\n",
    "##Feature engineering functions\n",
    "def feature_engg_features(features):\n",
    "  #Add new features( Non-TFT transformation ) -> Just for study purposes\n",
    "  features['distance'] = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5\n",
    "\n",
    "  return(features)\n",
    "\n",
    "#To be called from TF\n",
    "def feature_engg(features, label):\n",
    "  #Add new features\n",
    "  features = feature_engg_features(features)\n",
    "\n",
    "  return(features, label)\n",
    "\n",
    "###############################\n",
    "###Data Input pipeline function\n",
    "\n",
    "def make_input_fn(filename, mode, vnum_epochs = None, batch_size = 512):\n",
    "    def _input_fn(v_test=False):     \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.io.gfile.glob(filename)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this        \n",
    "        \n",
    "        # Create dataset from file list\n",
    "        dataset = tf.compat.v1.data.experimental.make_csv_dataset(file_list,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   column_names=CSV_COLUMNS,\n",
    "                                                   column_defaults=DEFAULTS,\n",
    "                                                   label_name=LABEL_COLUMN,\n",
    "                                                   num_epochs = num_epochs,\n",
    "                                                   num_parallel_reads=30)\n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size = batch_size)\n",
    "\n",
    "        #Feature engineering\n",
    "        dataset = dataset.map(feature_engg)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)       \n",
    "        \n",
    "        #Begins - Uncomment for testing only -----------------------------------------------------<\n",
    "        if v_test == True:\n",
    "          print(next(dataset.__iter__()))\n",
    "          \n",
    "        #End - Uncomment for testing only -----------------------------------------------------<\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "# Define feature columns(Including feature engineered ones )\n",
    "# These are the features which come from the TF Data pipeline\n",
    "def create_feature_cols():\n",
    "    #Keras format features\n",
    "    # k_pickup_longitude_scaled = tf.keras.Input(name='pickup_longitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    # k_pickup_latitude_scaled = tf.keras.Input(name='pickup_latitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n",
    "                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n",
    "                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance,\n",
    "                        # 'pickup_longitude_scaled': k_pickup_longitude_scaled,\n",
    "                        # 'pickup_latitude_scaled' : k_pickup_latitude_scaled\n",
    "                        }\n",
    "\n",
    "    return({'K' : keras_dict_input})\n",
    "\n",
    "def create_keras_model(params, feature_cols):\n",
    "    METRICS = [\n",
    "            keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "    ]\n",
    "\n",
    "    #Input layers\n",
    "    input_feats = []\n",
    "    for inp in feature_cols['K'].keys():\n",
    "      input_feats.append(feature_cols['K'][inp])\n",
    "\n",
    "    ##Input processing\n",
    "    ##https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "    ##https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md\n",
    "\n",
    "    ##Handle categorical attributes( One-hot encoding )\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary = ['0','1','2','3','4','5','6','7'], mask_token=None, oov_token = '0')(feature_cols['K']['trip_start_day'])\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=8)(cat_day)\n",
    "\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12','13','14','15','16',\n",
    "                                                                                      '17','18','19','20','21','22','23','0'\n",
    "                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=24)(cat_hour)\n",
    "\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=12)(cat_month)\n",
    "\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=len(df['company'].unique()))(cat_company)\n",
    "\n",
    "    ##Binning\n",
    "    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n",
    "    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n",
    "\n",
    "    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n",
    "    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n",
    "\n",
    "    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n",
    "    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n",
    "\n",
    "    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n",
    "    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n",
    "\n",
    "    ##Categorical cross\n",
    "    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n",
    "    # hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=24 * 8, output_mode='one_hot')(cross_day_hour)\n",
    "\n",
    "#     cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n",
    "#     hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n",
    "\n",
    "#     cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n",
    "#     hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n",
    "\n",
    "    # Cross to embedding\n",
    "#     embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n",
    "#     embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n",
    "\n",
    "#     embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n",
    "#     embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n",
    "\n",
    "    # Also pass time attributes as Deep signal( Cast to integer )\n",
    "    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n",
    "    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n",
    "    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n",
    "\n",
    "    #Add feature engineered columns - LAMBDA layer\n",
    "\n",
    "    ###Create MODEL\n",
    "    ####Concatenate all features( Numerical input )\n",
    "    x_input_numeric = tf.keras.layers.concatenate([\n",
    "                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n",
    "                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n",
    "                    # feature_cols['K']['pickup_latitude_scaled'], feature_cols['K']['pickup_longitude_scaled'],\n",
    "                    feature_cols['K']['distance'], \n",
    "                    # embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n",
    "                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n",
    "                    ])\n",
    "\n",
    "    #DEEP - This Dense layer connects to input layer - Numeric Data\n",
    "    # x_numeric = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_numeric)\n",
    "    x_numeric = tf.keras.layers.BatchNormalization()(x_input_numeric)\n",
    "\n",
    "    ####Concatenate all Categorical features( Categorical converted )\n",
    "    x_categ = tf.keras.layers.concatenate([\n",
    "                    cat_month, #cat_cross_day_hour, \n",
    "                    cat_pickup_lat, cat_pickup_lon,\n",
    "                    cat_drop_lat, cat_drop_lon\n",
    "                    ])\n",
    "    \n",
    "    #WIDE - This Dense layer connects to input layer - Categorical Data\n",
    "    # x_categ = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_categ)\n",
    "\n",
    "    ####Concatenate both Wide and Deep layers\n",
    "    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n",
    "\n",
    "    for l_ in range(params['hidden_layers']):\n",
    "        x = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\",\n",
    "                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #Final Layer\n",
    "    out = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(input_feats, out)\n",
    "\n",
    "    #Set optimizer\n",
    "    opt = tf.keras.optimizers.Adam(lr= params['lr'])\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n",
    "\n",
    "    #Print Summary\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n",
    "  #Add callbacks\n",
    "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=5, min_lr=0.00001, verbose = 1)\n",
    "  \n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\")\n",
    "\n",
    "  #Train and Evaluate\n",
    "  out = model.fit(train_dataset, \n",
    "                  validation_data = validation_dataset,\n",
    "                  epochs=epochs,\n",
    "                  # validation_steps = 3,   ###Keep this none for running evaluation on full EVAL data every epoch\n",
    "                  steps_per_epoch = 100,   ###Has to be passed - Cant help it :) [ Number of batches per epoch ]\n",
    "                  callbacks=[reduce_lr, #modelsave_callback, \n",
    "                             tensorboard_callback, \n",
    "                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
    "                  )\n",
    "  \n",
    "  return (min(out.history['val_rmse']))\n",
    "  \n",
    "def main(args):\n",
    "    @tf.function\n",
    "    def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n",
    "        #Params coming in request\n",
    "        features = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month\n",
    "        }\n",
    "\n",
    "        #Add TFT transformations\n",
    "        raw_features = {}\n",
    "        for key, val in features.items():\n",
    "          if key not in RAW_DATA_FEATURE_SPEC:\n",
    "            continue\n",
    "          if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
    "            raw_features[key] = tf.RaggedTensor.from_tensor(\n",
    "                tf.expand_dims(val, -1)).to_sparse()\n",
    "            continue\n",
    "          raw_features[key] = val\n",
    "        # tft_new_features = tft_layer(raw_features)\n",
    "\n",
    "        # pickup_longitude_scaled = tft_new_features['pickup_longitude_scaled'] \n",
    "        # pickup_latitude_scaled = tft_new_features['pickup_latitude_scaled']\n",
    "        distance = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5 ##tft_new_features['distance']\n",
    "\n",
    "        ##Feature engineering( calculate distance )\n",
    "        # distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n",
    "\n",
    "        #Params in request + New Feature engineering params\n",
    "        payload = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month,\n",
    "            'distance': distance,\n",
    "            # 'pickup_longitude_scaled': pickup_longitude_scaled,\n",
    "            # 'pickup_latitude_scaled': pickup_latitude_scaled,\n",
    "        }\n",
    "\n",
    "        ## Predict\n",
    "        ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, \"COULDN'T COMPUTE OUTPUT TENSOR\"\n",
    "        predictions = m_(payload)\n",
    "        return predictions\n",
    "\n",
    "    #####MAIN STARTS\n",
    "    ##Device Strategy\n",
    "    device = \"cpu\"\n",
    "    if device == \"tpu\":\n",
    "      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "      tf.config.experimental_connect_to_cluster(resolver)\n",
    "      # This is the TPU initialization code that has to be at the beginning.\n",
    "      tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "      strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "    else:\n",
    "      strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    \n",
    "    #Create FC\n",
    "    create_feature_cols()\n",
    "\n",
    "    #Create model\n",
    "    params_default = {\n",
    "        'lr' : args.lr,\n",
    "        'beta_1' : 0.99,\n",
    "        'beta_2' : 0.999,\n",
    "        'epsilon' : 1e-08,\n",
    "        'decay' : 0.01,\n",
    "        'hidden_layers' : args.hidden_layers\n",
    "    }\n",
    "\n",
    "    #Create dataset input functions\n",
    "    train_dataset = make_input_fn(filename = args.train_file,\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = 128)()\n",
    "\n",
    "    validation_dataset = make_input_fn(filename = args.eval_file,\n",
    "                        mode = tf.estimator.ModeKeys.EVAL,\n",
    "                        batch_size = 512)()\n",
    "\n",
    "    m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols())\n",
    "    # tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "    #Train Model\n",
    "    rmse = keras_train_and_evaluate(m_, train_dataset, validation_dataset, args.epochs)\n",
    "    print(\"Final Val RMSE: \", rmse)\n",
    "    \n",
    "    #Report metrics for HPT\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='eval_rmse',\n",
    "      metric_value=rmse,\n",
    "      global_step=args.epochs)\n",
    "    \n",
    "    #Save model\n",
    "    serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n",
    "                                            trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n",
    "                                            trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n",
    "                                            dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n",
    "                                            dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n",
    "                                            pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n",
    "                                            pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n",
    "                                            )\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    version = \"1\"  #{'serving_default': call_output}\n",
    "    tf.saved_model.save(\n",
    "        m_,\n",
    "        args.model_save_location + version,\n",
    "        signatures=serving\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--train_file', required=True, type=str, help='Training file')\n",
    "    parser.add_argument(\n",
    "      '--eval_file', required=True, type=str, help='Eval file')\n",
    "    parser.add_argument(\n",
    "      '--model_save_location', required=True, type=str, help='Model save location')\n",
    "    parser.add_argument(\n",
    "      '--epochs', required=False, type=int, help='Epochs', default=100)\n",
    "    parser.add_argument(\n",
    "      '--lr', required=False, type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument(\n",
    "      '--hidden_layers', required=False, type=int, help='Hidden layers', default=1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #Run Main Trainer\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e4f53-5a4d-461e-971e-51d21e89ccaa",
   "metadata": {},
   "source": [
    "### Test task.py locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e2827a-87db-4371-995f-d5cff55422a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 12:53:22.825845: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_start_month (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_longitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " trip_start_day (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_start_hour (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      "                                                                                                  \n",
      " discretization (Discretization  (None, 1)           0           ['pickup_latitude[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " discretization_1 (Discretizati  (None, 1)           0           ['pickup_longitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_2 (Discretizati  (None, 1)           0           ['dropoff_latitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_3 (Discretizati  (None, 1)           0           ['dropoff_longitude[0][0]']      \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " distance (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.strings.to_number (TFOpLamb  (None, 1)           0           ['trip_start_day[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.strings.to_number_1 (TFOpLa  (None, 1)           0           ['trip_start_hour[0][0]']        \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.strings.to_number_2 (TFOpLa  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 12)          0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_3 (CategoryE  (None, 21)          0           ['discretization[0][0]']         \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_4 (CategoryE  (None, 21)          0           ['discretization_1[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_5 (CategoryE  (None, 21)          0           ['discretization_2[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_6 (CategoryE  (None, 21)          0           ['discretization_3[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8)            0           ['pickup_latitude[0][0]',        \n",
      "                                                                  'pickup_longitude[0][0]',       \n",
      "                                                                  'dropoff_latitude[0][0]',       \n",
      "                                                                  'dropoff_longitude[0][0]',      \n",
      "                                                                  'distance[0][0]',               \n",
      "                                                                  'tf.strings.to_number[0][0]',   \n",
      "                                                                  'tf.strings.to_number_1[0][0]', \n",
      "                                                                  'tf.strings.to_number_2[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 96)           0           ['category_encoding_2[0][0]',    \n",
      "                                                                  'category_encoding_3[0][0]',    \n",
      "                                                                  'category_encoding_4[0][0]',    \n",
      "                                                                  'category_encoding_5[0][0]',    \n",
      "                                                                  'category_encoding_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8)           32          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 104)          0           ['concatenate_1[0][0]',          \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           3360        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            33          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,473\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 4s 22ms/step - loss: 161.7065 - rmse: 12.7164 - val_loss: 4228.1367 - val_rmse: 65.0239 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 107.2511 - rmse: 10.3562 - val_loss: 671.7133 - val_rmse: 25.9173 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 77.7907 - rmse: 8.8199 - val_loss: 146.9419 - val_rmse: 12.1218 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 43.2144 - rmse: 6.5738 - val_loss: 51.9596 - val_rmse: 7.2082 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 23.8596 - rmse: 4.8846 - val_loss: 43.0843 - val_rmse: 6.5638 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 17.5210 - rmse: 4.1858 - val_loss: 35.8423 - val_rmse: 5.9868 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 16.5651 - rmse: 4.0700 - val_loss: 22.2164 - val_rmse: 4.7134 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 16.1198 - rmse: 4.0149 - val_loss: 20.9391 - val_rmse: 4.5759 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 16.1696 - rmse: 4.0211 - val_loss: 20.3160 - val_rmse: 4.5073 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 14.5815 - rmse: 3.8186 - val_loss: 20.2688 - val_rmse: 4.5021 - lr: 0.0010\n",
      "Epoch 11/100\n",
      " 39/100 [==========>...................] - ETA: 0s - loss: 14.6006 - rmse: 3.8211^C\n",
      "Traceback (most recent call last):\n",
      "  File \"task.py\", line 397, in <module>\n",
      "    main(args)\n",
      "  File \"task.py\", line 351, in main\n",
      "    rmse = keras_train_and_evaluate(m_, train_dataset, validation_dataset, args.epochs)\n",
      "  File \"task.py\", line 256, in keras_train_and_evaluate\n",
      "    keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1216, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 910, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 942, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3131, in __call__\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 603, in call\n",
      "    ctx=ctx)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python task.py \\\n",
    "--train_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--eval_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--model_save_location='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--epochs=3 \\\n",
    "--lr=0.001 \\\n",
    "--hidden_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af8357-e285-4c3e-85c3-23f7d0a9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d207a-b96b-4bb3-8488-e15909b2428e",
   "metadata": {},
   "source": [
    "### Test task.py Gcloud Local Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb3481f-90ee-41a4-9ca2-6691b7fd0a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package is set to /home/jupyter.\n",
      "Sending build context to Docker daemon   33.8MB\n",
      "Step 1/12 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest\n",
      " ---> 7dc88f3ac4e2\n",
      "Step 2/12 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in 0b4ac9ca389b\n",
      "Removing intermediate container 0b4ac9ca389b\n",
      " ---> 00ff2a8b2176\n",
      "Step 3/12 : WORKDIR /usr/app\n",
      " ---> Running in a14147673617\n",
      "Removing intermediate container a14147673617\n",
      " ---> 65dde0dfc4ce\n",
      "Step 4/12 : ENV HOME=/home\n",
      " ---> Running in c82959819862\n",
      "Removing intermediate container c82959819862\n",
      " ---> 7b6c44904df1\n",
      "Step 5/12 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 21d6eb032407\n",
      "Removing intermediate container 21d6eb032407\n",
      " ---> 6460d4394084\n",
      "Step 6/12 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in a70f90745d8a\n",
      "Removing intermediate container a70f90745d8a\n",
      " ---> 5bac260fb8a0\n",
      "Step 7/12 : COPY [\"./setup.py\", \"./setup.py\"]\n",
      " ---> 2218fd57196e\n",
      "Step 8/12 : RUN pip3 install --no-cache-dir .\n",
      " ---> Running in f474a7322a0d\n",
      "Processing /usr/app\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (0.15.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (1.3.4)\n",
      "Collecting google-cloud\n",
      "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (1.42.3)\n",
      "Requirement already satisfied: google-cloud-firestore in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (2.3.4)\n",
      "Requirement already satisfied: google-api-python-client in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (2.29.0)\n",
      "Requirement already satisfied: google-auth in /opt/conda/lib/python3.7/site-packages (from trainer==0.1) (2.3.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->trainer==0.1) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client->trainer==0.1) (0.20.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client->trainer==0.1) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client->trainer==0.1) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client->trainer==0.1) (2.2.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth->trainer==0.1) (58.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth->trainer==0.1) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth->trainer==0.1) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth->trainer==0.1) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-firestore->trainer==0.1) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-firestore->trainer==0.1) (21.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-firestore->trainer==0.1) (1.19.7)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->trainer==0.1) (2.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->trainer==0.1) (2.25.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->trainer==0.1) (3.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->trainer==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->trainer==0.1) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->trainer==0.1) (1.19.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client->trainer==0.1) (1.53.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client->trainer==0.1) (1.41.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client->trainer==0.1) (1.41.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->trainer==0.1) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->trainer==0.1) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth->trainer==0.1) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->trainer==0.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->trainer==0.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->trainer==0.1) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->trainer==0.1) (2.10)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->trainer==0.1) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->trainer==0.1) (2.20)\n",
      "Building wheels for collected packages: trainer\n",
      "  Building wheel for trainer (setup.py): started\n",
      "  Building wheel for trainer (setup.py): finished with status 'done'\n",
      "  Created wheel for trainer: filename=trainer-0.1-py3-none-any.whl size=1047 sha256=82b0033a2acb0193061544433519af9e91147c13a2dcef24363e6fcd54f269c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3_dq5qp6/wheels/12/03/c9/f722c02b66385adff7c7fa6d490848e7b252673926e52cfa41\n",
      "Successfully built trainer\n",
      "Installing collected packages: google-cloud, trainer\n",
      "Successfully installed google-cloud-0.34.0 trainer-0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container f474a7322a0d\n",
      " ---> b9ee767d01bd\n",
      "Step 9/12 : COPY [\"./requirements.txt\", \"./requirements.txt\"]\n",
      " ---> c9f6e3071e40\n",
      "Step 10/12 : RUN pip3 install --no-cache-dir -r ./requirements.txt\n",
      " ---> Running in 55226e136fec\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /opt/conda/lib/python3.7/site-packages (from -r ./requirements.txt (line 1)) (2.7.0)\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from -r ./requirements.txt (line 3)) (3.4.3)\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (from -r ./requirements.txt (line 4)) (0.1.0.dev6)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: pydot in /opt/conda/lib/python3.7/site-packages (from -r ./requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.21.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.41.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (12.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.5->-r ./requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.5->-r ./requirements.txt (line 2)) (2021.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->-r ./requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (58.5.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (4.8.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r ./requirements.txt (line 1)) (3.6.0)\n",
      "Installing collected packages: pandas, graphviz\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "Successfully installed graphviz-0.19.1 pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.29.0 which is incompatible.\n",
      "tfx-bsl 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 6.0.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 6.0.0 which is incompatible.\n",
      "tensorflow-transform 1.3.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2, but you have tensorflow 2.7.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 55226e136fec\n",
      " ---> 0c72a21a81d5\n",
      "Step 11/12 : COPY [\".\", \".\"]\n",
      " ---> d00f5074dc4e\n",
      "Step 12/12 : ENTRYPOINT [\"python3\", \"task.py\"]\n",
      " ---> Running in 6fcbdede176f\n",
      "Removing intermediate container 6fcbdede176f\n",
      " ---> ae6ed633f95a\n",
      "Successfully built ae6ed633f95a\n",
      "Successfully tagged cloudai-autogenerated/task.py:20211228.12.09.36.236295\n",
      "A training image is built.\n",
      "Starting to run ...\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_start_month (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_longitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " trip_start_day (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_start_hour (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      "                                                                                                  \n",
      " discretization (Discretization  (None, 1)           0           ['pickup_latitude[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " discretization_1 (Discretizati  (None, 1)           0           ['pickup_longitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_2 (Discretizati  (None, 1)           0           ['dropoff_latitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_3 (Discretizati  (None, 1)           0           ['dropoff_longitude[0][0]']      \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " distance (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.strings.to_number (TFOpLamb  (None, 1)           0           ['trip_start_day[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.strings.to_number_1 (TFOpLa  (None, 1)           0           ['trip_start_hour[0][0]']        \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.strings.to_number_2 (TFOpLa  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 12)          0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_3 (CategoryE  (None, 21)          0           ['discretization[0][0]']         \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_4 (CategoryE  (None, 21)          0           ['discretization_1[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_5 (CategoryE  (None, 21)          0           ['discretization_2[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_6 (CategoryE  (None, 21)          0           ['discretization_3[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8)            0           ['pickup_latitude[0][0]',        \n",
      "                                                                  'pickup_longitude[0][0]',       \n",
      "                                                                  'dropoff_latitude[0][0]',       \n",
      "                                                                  'dropoff_longitude[0][0]',      \n",
      "                                                                  'distance[0][0]',               \n",
      "                                                                  'tf.strings.to_number[0][0]',   \n",
      "                                                                  'tf.strings.to_number_1[0][0]', \n",
      "                                                                  'tf.strings.to_number_2[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 96)           0           ['category_encoding_2[0][0]',    \n",
      "                                                                  'category_encoding_3[0][0]',    \n",
      "                                                                  'category_encoding_4[0][0]',    \n",
      "                                                                  'category_encoding_5[0][0]',    \n",
      "                                                                  'category_encoding_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8)           32          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 104)          0           ['concatenate_1[0][0]',          \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           3360        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           1056        ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,593\n",
      "Non-trainable params: 144\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 15ms/step - loss: 154.9023 - rmse: 12.4460 - val_loss: 343.0135 - val_rmse: 18.5200 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 106.9017 - rmse: 10.3393 - val_loss: 108.8018 - val_rmse: 10.4302 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 73.2458 - rmse: 8.5583 - val_loss: 166.7188 - val_rmse: 12.9116 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 42.0893 - rmse: 6.4876 - val_loss: 80.1915 - val_rmse: 8.9547 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 24.3374 - rmse: 4.9332 - val_loss: 58.5772 - val_rmse: 7.6535 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 15.7737 - rmse: 3.9716 - val_loss: 33.4374 - val_rmse: 5.7824 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 17.4313 - rmse: 4.1750 - val_loss: 25.3612 - val_rmse: 5.0359 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 13.5751 - rmse: 3.6844 - val_loss: 19.8230 - val_rmse: 4.4522 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.8921 - rmse: 3.8590 - val_loss: 19.9937 - val_rmse: 4.4714 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.4725 - rmse: 3.8042 - val_loss: 19.9100 - val_rmse: 4.4620 - lr: 0.0010\n",
      "2021-12-28 12:10:43.911542: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "Final Val RMSE:  4.452248573303223\n",
      "Saving model...\n",
      "A local run is finished successfully using custom image: cloudai-autogenerated/task.py:20211228.12.09.36.236295.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs local-run \\\n",
    "--executor-image-uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest' \\\n",
    "--script=task.py \\\n",
    "-- \\\n",
    "--train_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--eval_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--model_save_location='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--epochs=10 \\\n",
    "--lr=0.001 \\\n",
    "--hidden_layers=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a6372-b9fa-4304-b171-641023d59fc7",
   "metadata": {},
   "source": [
    "### Prebuilt container - GCloud Custom Training Job\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/create-custom-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb59d3de-4578-433f-8a39-29e98e74b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['absl-py','pandas',\n",
    "                     'google-cloud','google-cloud-storage','google-cloud-firestore','google-api-python-client', 'google-auth']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Hasan - Vertex AI Taxi Trainer Job'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60e9dbb5-813e-4a5f-b6f7-fccc2f3dfe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db93eb12-3ed4-4e98-b14c-4df5a431aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n",
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  5.9 KiB/  5.9 KiB]                                                \n",
      "Operation completed over 1 objects/5.9 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Create ML train package\n",
    "!rm -rf dist*\n",
    "!rm -rf trainer*\n",
    "!rm -rf trainer/\n",
    "!rm -rf dist/\n",
    "\n",
    "!mkdir trainer/\n",
    "!cp task.py trainer/\n",
    "!cp __init__.py trainer/\n",
    "!python setup.py sdist\n",
    "\n",
    "# Copy trainer.gz to GCS training path\n",
    "!gsutil cp dist/trainer-0.1.tar.gz gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78463541-6238-4b84-bd4e-83a0692fdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/318948681665/locations/us-central1/customJobs/6510310602643079168] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/318948681665/locations/us-central1/customJobs/6510310602643079168\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/318948681665/locations/us-central1/customJobs/6510310602643079168\n"
     ]
    }
   ],
   "source": [
    "##Launch Job on GCLOUD\n",
    "!gcloud ai custom-jobs create \\\n",
    "--region=us-central1 \\\n",
    "--display-name=vertex-custom-taxi5 \\\n",
    "--python-package-uris=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/trainer-0.1.tar.gz \\\n",
    "--worker-pool-spec=machine-type=n1-standard-4,executor-image-uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest',python-module=trainer.task \\\n",
    "--args='--train_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--args='--eval_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--args='--model_save_location=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--args='--epochs=10' \\\n",
    "--args='--hidden_layers=2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4d219-e081-4dc3-9607-6930a76aca63",
   "metadata": {},
   "source": [
    "### Prebuilt container - GCloud HPT Job\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "827fe436-8a40-4b6c-9828-cc09cf0ccd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: eval_rmse\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: lr\n",
    "    doubleValueSpec:\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.05\n",
    "  - parameterId: hidden_layers\n",
    "    integerValueSpec:\n",
    "      minValue: 1\n",
    "      maxValue: 4\n",
    "  algorithm: RANDOM_SEARCH\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-standard-4\n",
    "    replicaCount: \"1\"\n",
    "    pythonPackageSpec: \n",
    "        packageUris: gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/trainer-0.1.tar.gz\n",
    "        pythonModule: trainer.task\n",
    "        executorImageUri: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest\n",
    "        args: [\n",
    "          \"--train_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv\",\n",
    "          \"--eval_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv\",\n",
    "          \"--model_save_location=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/\"\n",
    "        ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f1facbf-29d3-443b-83cb-857d51b18efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [6260923773277437952] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 6260923773277437952 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai hp-tuning-jobs create \\\n",
    "--region=us-central1 \\\n",
    "--display-name=vertex-custom-hpt-taxi1 \\\n",
    "--max-trial-count=10 \\\n",
    "--parallel-trial-count=2 \\\n",
    "--config=config.yaml \\\n",
    "# --verbosity=debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ed279-c20b-4df5-aeaa-fce9212c017b",
   "metadata": {},
   "source": [
    "### Custom container - GCloud Custom Training Job\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/create-custom-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e8489d6-fd11-47bc-b48d-e5fe97fc6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow==2.7.0\n",
    "pandas==1.3.5\n",
    "matplotlib\n",
    "cloudml-hypertune\n",
    "graphviz\n",
    "pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "453fb63e-ca1d-439b-b3b1-82024c12aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "##Build Dockerfile\n",
    "FROM python:3.7-slim AS builder\n",
    "COPY requirements.txt .\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"task.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50f29780-981c-4831-8994-fd49a596db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "#Get access to write to GAR\n",
    "!gcloud auth configure-docker us-central1-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "29c64793-3dec-429a-9f7d-6c7cde666ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  34.14MB\n",
      "Step 1/5 : FROM python:3.7-slim AS builder\n",
      " ---> d3c9ad326043\n",
      "Step 2/5 : COPY requirements.txt .\n",
      " ---> Using cache\n",
      " ---> 5f9591bbc943\n",
      "Step 3/5 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 463635d3dacb\n",
      "Step 4/5 : COPY . .\n",
      " ---> 8c9941fb103f\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Running in 9f7c3ac03341\n",
      "Removing intermediate container 9f7c3ac03341\n",
      " ---> f1225905dc39\n",
      "Successfully built f1225905dc39\n",
      "Successfully tagged us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest\n"
     ]
    }
   ],
   "source": [
    "## Build Docker image\n",
    "!docker build -t us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7da30761-eee4-4e13-8954-a1c722cfada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-28 12:54:20.223260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-28 12:54:20.223310: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-28 12:54:22.005427: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-28 12:54:22.005482: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-28 12:54:22.005512: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7154c91bb4ee): /proc/driver/nvidia/version does not exist\n",
      "2021-12-28 12:54:22.006113: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_start_month (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_longitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " trip_start_day (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_start_hour (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      "                                                                                                  \n",
      " discretization (Discretization  (None, 1)           0           ['pickup_latitude[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " discretization_1 (Discretizati  (None, 1)           0           ['pickup_longitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_2 (Discretizati  (None, 1)           0           ['dropoff_latitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_3 (Discretizati  (None, 1)           0           ['dropoff_longitude[0][0]']      \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " distance (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.strings.to_number (TFOpLamb  (None, 1)           0           ['trip_start_day[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.strings.to_number_1 (TFOpLa  (None, 1)           0           ['trip_start_hour[0][0]']        \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.strings.to_number_2 (TFOpLa  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 12)          0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_3 (CategoryE  (None, 21)          0           ['discretization[0][0]']         \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_4 (CategoryE  (None, 21)          0           ['discretization_1[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_5 (CategoryE  (None, 21)          0           ['discretization_2[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_6 (CategoryE  (None, 21)          0           ['discretization_3[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8)            0           ['pickup_latitude[0][0]',        \n",
      "                                                                  'pickup_longitude[0][0]',       \n",
      "                                                                  'dropoff_latitude[0][0]',       \n",
      "                                                                  'dropoff_longitude[0][0]',      \n",
      "                                                                  'distance[0][0]',               \n",
      "                                                                  'tf.strings.to_number[0][0]',   \n",
      "                                                                  'tf.strings.to_number_1[0][0]', \n",
      "                                                                  'tf.strings.to_number_2[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 96)           0           ['category_encoding_2[0][0]',    \n",
      "                                                                  'category_encoding_3[0][0]',    \n",
      "                                                                  'category_encoding_4[0][0]',    \n",
      "                                                                  'category_encoding_5[0][0]',    \n",
      "                                                                  'category_encoding_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8)           32          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 104)          0           ['concatenate_1[0][0]',          \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           3360        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           1056        ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,593\n",
      "Non-trainable params: 144\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 14ms/step - loss: 163.4638 - rmse: 12.7853 - val_loss: 304.3839 - val_rmse: 17.4457 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 106.5389 - rmse: 10.3218 - val_loss: 116.6221 - val_rmse: 10.7986 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 77.8196 - rmse: 8.8215 - val_loss: 123.7164 - val_rmse: 11.1224 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 45.5863 - rmse: 6.7517 - val_loss: 112.9368 - val_rmse: 10.6270 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 26.0816 - rmse: 5.1070 - val_loss: 70.2616 - val_rmse: 8.3821 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 15.2240 - rmse: 3.9017 - val_loss: 38.4582 - val_rmse: 6.2014 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 15.8099 - rmse: 3.9761 - val_loss: 24.6015 - val_rmse: 4.9599 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.6634 - rmse: 3.8292 - val_loss: 20.7136 - val_rmse: 4.5512 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 15.4416 - rmse: 3.9295 - val_loss: 19.8072 - val_rmse: 4.4505 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 13.8665 - rmse: 3.7237 - val_loss: 19.6330 - val_rmse: 4.4309 - lr: 0.0010\n",
      "2021-12-28 12:54:34.194913: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/usr/local/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "Final Val RMSE:  4.430855751037598\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "## Test run docker image in local\n",
    "!docker run 'us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest' \\\n",
    "--train_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--eval_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--model_save_location='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--epochs=10 \\\n",
    "--lr=0.001 \\\n",
    "--hidden_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac5bf998-7682-4aeb-9a65-274b9a231f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest]\n",
      "\n",
      "\u001b[1B72c73987: Preparing \n",
      "\u001b[1B115bd59e: Preparing \n",
      "\u001b[1B65b7decb: Preparing \n",
      "\u001b[1B0307b4c1: Preparing \n",
      "\u001b[1B45955cb1: Preparing \n",
      "\u001b[1B23303735: Preparing \n",
      "\u001b[1B20bfdce7: Preparing \n",
      "\u001b[8B72c73987: Pushed   34.14MB/32.45MB\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2Klatest: digest: sha256:6fe3155710c3d76ce2a9e5c17188552dd2b22f3858ef7c88dc48f0dfb40540fb size: 2003\n"
     ]
    }
   ],
   "source": [
    "## Push Docker image to GAR\n",
    "!docker push us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81c8b39d-000e-479d-9966-56dc9b1aed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/318948681665/locations/us-central1/customJobs/217093073346232320] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/318948681665/locations/us-central1/customJobs/217093073346232320\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/318948681665/locations/us-central1/customJobs/217093073346232320\n"
     ]
    }
   ],
   "source": [
    "##Launch Job on GCLOUD\n",
    "!gcloud ai custom-jobs create \\\n",
    "--region=us-central1 \\\n",
    "--display-name=vertex-custom-taxicustomcontainer-3 \\\n",
    "--worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri='us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest' \\\n",
    "--args='--train_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--args='--eval_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--args='--model_save_location=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--args='--epochs=10' \\\n",
    "--args='--hidden_layers=2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849aa07-eaab-4862-afc9-fbea4793dfc2",
   "metadata": {},
   "source": [
    "### Custom container - GCloud HPT Job\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17882fd1-706e-42c2-8f58-3a0e9a6b4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config_custom_container.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config_custom_container.yaml\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: eval_rmse\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: lr\n",
    "    doubleValueSpec:\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.05\n",
    "  - parameterId: hidden_layers\n",
    "    integerValueSpec:\n",
    "      minValue: 1\n",
    "      maxValue: 4\n",
    "  algorithm: RANDOM_SEARCH\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-standard-4\n",
    "    replicaCount: \"1\"\n",
    "    containerSpec:\n",
    "        imageUri: us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-custom-training-docker/latest:latest\n",
    "        args: [\n",
    "          \"--train_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv\",\n",
    "          \"--eval_file=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv\",\n",
    "          \"--model_save_location=gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/\"\n",
    "          ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d9f6196-3c34-4efa-b198-81ddd8481438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [7783140447328665600] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 7783140447328665600 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai hp-tuning-jobs create \\\n",
    "--region=us-central1 \\\n",
    "--display-name=vertex-customcontainer-hpt-taxi1 \\\n",
    "--max-trial-count=10 \\\n",
    "--parallel-trial-count=2 \\\n",
    "--config=config_custom_container.yaml \\\n",
    "# --verbosity=debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea74dd8-0962-46e2-a322-9aedde4bfa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
