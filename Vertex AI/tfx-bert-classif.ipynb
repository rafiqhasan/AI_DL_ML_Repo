{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58yt61n9MDdU"
   },
   "source": [
    "# NLP ML on TFX with Vertex Pipelines\n",
    "\n",
    "This pipeline demonstrates data preprocessing, training, and export of a classification based on the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rinax0YJ_otk"
   },
   "source": [
    "# Project Setup\n",
    "\n",
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sjjgiv0bM0hi",
    "outputId": "f75d460a-3f4b-477b-c3db-99aed7da21c8"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq --use-deprecated=legacy-resolver tfx\n",
    "!pip install -Uq tensorflow-text  # The tf-text version needs to match the tf version\n",
    "\n",
    "print(\"Restart your runtime enable after installing the packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NQSFUNALhFG"
   },
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oThi-x8xLlv-"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import tempfile\n",
    "from shutil import rmtree\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tfx\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "# from tensorflow_transform.tf_metadata import (dataset_metadata, dataset_schema,\n",
    "#                                               metadata_io, schema_utils)\n",
    "from tfx.components import (Evaluator, ExampleValidator, ImportExampleGen,\n",
    "                            ModelValidator, Pusher, SchemaGen,\n",
    "                            StatisticsGen, Trainer, Transform)\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.proto import evaluator_pb2, example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "# from tfx.utils.dsl_utils import external_input\n",
    "from tfx.dsl.components.common import resolver\n",
    "from typing import Dict, List, Text\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "from tfx.extensions.google_cloud_ai_platform.trainer import component as gcp_trainer\n",
    "from tfx.extensions.google_cloud_ai_platform.tuner import component as gcp_tuner\n",
    "from tfx.extensions.google_cloud_ai_platform.pusher import component as gcp_pusher\n",
    "from tfx.extensions.google_cloud_ai_platform.constants import ENABLE_VERTEX_KEY\n",
    "from tfx.extensions.google_cloud_ai_platform.constants import VERTEX_REGION_KEY\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_text as text\n",
    "from setuptools import distutils\n",
    "\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import \\\n",
    "    InteractiveContext\n",
    "\n",
    "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4OMItPSLnDj"
   },
   "source": [
    "## Download the IMDB Dataset from TensorFlow Datasets\n",
    "\n",
    "For our demo example, we are using the [IMDB data set](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) to train a sentiment model based on the pre-trained BERT model. The data set is provided through [TensorFlow Datasets](https://www.tensorflow.org/datasets). Our ML pipeline can read TFRecords, however it expects only TFRecord files in the data folder so removing JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tfds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjWjnzPGKjIk",
    "outputId": "ca32ed02-1942-4867-b451-6636841f3ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to tfds/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to tfds/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Deleted 4 files\n",
      "PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/318948681665/locations/us-central1/pipelineJobs/tfx-bert-classif-20230129103749 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!mkdir tfds/\n",
    "\n",
    "def clean_before_download(base_data_dir):\n",
    "    rmtree(base_data_dir)\n",
    "    \n",
    "def delete_unnecessary_files(base_path):\n",
    "    counter = 0\n",
    "    file_list = [\"dataset_info.json\", \"label.labels.txt\", \"features.json\"]\n",
    "\n",
    "    for f in file_list:\n",
    "        try:\n",
    "            os.remove(os.path.join(base_path, f))\n",
    "            counter += 1\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    for f in glob.glob(base_path + \"imdb_reviews-unsupervised.*\"):\n",
    "        os.remove(f)\n",
    "        counter += 1\n",
    "    print(f\"Deleted {counter} files\")\n",
    "\n",
    "def get_dataset(name='imdb_reviews', version=\"1.0.0\"):\n",
    "\n",
    "    base_data_dir = \"tfds/\"\n",
    "    config=\"plain_text\"\n",
    "    version=\"1.0.0\"\n",
    "\n",
    "    clean_before_download(base_data_dir)\n",
    "    tfds.disable_progress_bar()\n",
    "    builder = tfds.text.IMDBReviews(data_dir=base_data_dir, \n",
    "                                    config=config, \n",
    "                                    version=version)\n",
    "    download_config = tfds.download.DownloadConfig(\n",
    "        download_mode=tfds.GenerateMode.FORCE_REDOWNLOAD)\n",
    "    builder.download_and_prepare(download_config=download_config)\n",
    "\n",
    "    base_tfrecords_filename = os.path.join(base_data_dir, \"imdb_reviews\", config, version, \"\")\n",
    "    train_tfrecords_filename = base_tfrecords_filename + \"imdb_reviews-train*\"\n",
    "    test_tfrecords_filename = base_tfrecords_filename + \"imdb_reviews-test*\"\n",
    "    label_filename = os.path.join(base_tfrecords_filename, \"label.labels.txt\")\n",
    "    labels = [label.rstrip('\\n') for label in open(label_filename)]\n",
    "    delete_unnecessary_files(base_tfrecords_filename)\n",
    "    return (train_tfrecords_filename, test_tfrecords_filename), labels\n",
    "\n",
    "tfrecords_filenames, labels = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-5QGnm_lFJD"
   },
   "source": [
    "# TFX Pipeline\n",
    "\n",
    "The TensorFlow Extended Pipeline is more or less following the example setup shown here. We'll only note deviations from the original setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arPCEBYEFqEr"
   },
   "source": [
    "## Initializing the Interactive TFX Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBO0T3D5kkOt",
    "outputId": "11ed9aca-26ae-418a-d855-70cea36dd08d"
   },
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo2Q-c_ynL2x"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "W8GqUHwAKm6j",
    "outputId": "64a15e7b-f566-466b-8c13-003374f9583f"
   },
   "outputs": [],
   "source": [
    "output = example_gen_pb2.Output(\n",
    "             split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "                 example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=45),\n",
    "                 example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=5)\n",
    "             ]))\n",
    "# Load the data from our prepared TFDS folder\n",
    "data_root = \"tfds/imdb_reviews/plain_text/1.0.0/\"\n",
    "example_gen = ImportExampleGen(input_base=data_root, \n",
    "                               output_config=output)\n",
    "\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE9VL-pmF6L_"
   },
   "source": [
    "## TensorFlow Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EglytaKVLQzr",
    "outputId": "20d26d27-d3ed-4d87-e0da-825db987fc3c"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)\n",
    "\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "IBYoEPhBeQUi",
    "outputId": "5b2554db-0b45-459c-a1fd-2f047d0f5097"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=True)\n",
    "context.run(schema_gen)\n",
    "\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bl2gkqytjr0w",
    "outputId": "d2e637e3-496f-42f6-ac43-2cfeacbef61d"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "\n",
    "# check the data schema for the type of input tensors\n",
    "tfdv.load_schema_text(schema_gen.outputs['schema'].get()[0].uri + \"/schema.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "IWRswNYye6So",
    "outputId": "3ced1812-3ca8-4a5c-990d-6b6fb7c75dd1"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)\n",
    "\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zqjzTx2s5HS"
   },
   "source": [
    "## TensorFlow Transform\n",
    "\n",
    "This is where we perform the BERT processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K91irJq7q6vC",
    "outputId": "c6ee17b9-3b37-4ffd-a943-e0fe3fc8d0f4"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "%%writefile transform.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "MAX_SEQ_LEN = 512  # max number is 512\n",
    "BERT_TFHUB_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "\n",
    "def load_bert_layer(model_url=BERT_TFHUB_URL):\n",
    "    # Load the pre-trained BERT model as layer in Keras\n",
    "    bert_layer = hub.KerasLayer(\n",
    "        handle=model_url,\n",
    "        trainable=False)\n",
    "    return bert_layer\n",
    "\n",
    "do_lower_case = load_bert_layer().resolved_object.do_lower_case.numpy()\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input column of text into transformed columns of.\n",
    "        * input token ids\n",
    "        * input mask\n",
    "        * input type ids\n",
    "    \"\"\"\n",
    "\n",
    "    CLS_ID = tf.constant(101, dtype=tf.int64)\n",
    "    SEP_ID = tf.constant(102, dtype=tf.int64)\n",
    "    PAD_ID = tf.constant(0, dtype=tf.int64)\n",
    "\n",
    "    vocab_file_path = load_bert_layer().resolved_object.vocab_file.asset_path\n",
    "    \n",
    "    bert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_file_path, \n",
    "                                        token_out_type=tf.int64, \n",
    "                                        lower_case=do_lower_case) \n",
    "    \n",
    "    def tokenize_text(text, sequence_length=MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        Perform the BERT preprocessing from text -> input token ids\n",
    "        \"\"\"\n",
    "\n",
    "        # convert text into token ids\n",
    "        tokens = bert_tokenizer.tokenize(text)\n",
    "        \n",
    "        # flatten the output ragged tensors \n",
    "        tokens = tokens.merge_dims(1, 2)[:, :sequence_length]\n",
    "        \n",
    "        # Add start and end token ids to the id sequence\n",
    "        start_tokens = tf.fill([tf.shape(text)[0], 1], CLS_ID)\n",
    "        end_tokens = tf.fill([tf.shape(text)[0], 1], SEP_ID)\n",
    "        tokens = tokens[:, :sequence_length - 2]\n",
    "        tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)\n",
    "\n",
    "        # truncate sequences greater than MAX_SEQ_LEN\n",
    "        tokens = tokens[:, :sequence_length]\n",
    "\n",
    "        # pad shorter sequences with the pad token id\n",
    "        tokens = tokens.to_tensor(default_value=PAD_ID)\n",
    "        pad = sequence_length - tf.shape(tokens)[1]\n",
    "        tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n",
    "\n",
    "        # and finally reshape the word token ids to fit the output \n",
    "        # data structure of TFT  \n",
    "        return tf.reshape(tokens, [-1, sequence_length])\n",
    "\n",
    "    def preprocess_bert_input(text):\n",
    "        \"\"\"\n",
    "        Convert input text into the input_word_ids, input_mask, input_type_ids\n",
    "        \"\"\"\n",
    "        input_word_ids = tokenize_text(text)\n",
    "        input_mask = tf.cast(input_word_ids > 0, tf.int64)\n",
    "        input_mask = tf.reshape(input_mask, [-1, MAX_SEQ_LEN])\n",
    "        \n",
    "        zeros_dims = tf.stack(tf.shape(input_mask))\n",
    "        input_type_ids = tf.fill(zeros_dims, 0)\n",
    "        input_type_ids = tf.cast(input_type_ids, tf.int64)\n",
    "\n",
    "        return (\n",
    "            input_word_ids, \n",
    "            input_mask,\n",
    "            input_type_ids\n",
    "        )\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = \\\n",
    "        preprocess_bert_input(tf.squeeze(inputs['text'], axis=1))\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': input_type_ids,\n",
    "        'label': inputs['label']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Sz5cevHYrR6M",
    "outputId": "2f16b7a9-0705-423e-80ae-dfe5984d9973"
   },
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(\"transform.py\"))\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCcNJxWSKPPv"
   },
   "source": [
    "#### Check the Output Data Struture of the TF Transform Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcUEGmuhtmGi",
    "outputId": "691e931b-fc6c-42af-feb3-34ce0c3ab631"
   },
   "outputs": [],
   "source": [
    "from tfx_bsl.coders.example_coder import ExampleToNumpyDict\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "train_uri = transform.outputs['transformed_examples'].get()[0].uri\n",
    "\n",
    "print(train_uri)\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_folders = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
    "tfrecord_filenames = []\n",
    "for tfrecord_folder in tfrecord_folders:\n",
    "    for name in os.listdir(tfrecord_folder):\n",
    "        tfrecord_filenames.append(os.path.join(tfrecord_folder, name))\n",
    "\n",
    "\n",
    "# Create a TFRecordDataset to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "for tfrecord in dataset.take(1):\n",
    "    serialized_example = tfrecord.numpy()\n",
    "    example = ExampleToNumpyDict(serialized_example)\n",
    "    pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdsXSG52kVoL"
   },
   "source": [
    "## Training of the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywjksr-vtxrX",
    "outputId": "7266cd41-2596-40a3-99a9-c29e570fd7c1"
   },
   "outputs": [],
   "source": [
    "%%skip_for_export\n",
    "%%writefile trainer.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from typing import Text\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_transform as tft\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "LABEL_KEY = 'label'\n",
    "BERT_TFHUB_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "\n",
    "def load_bert_layer(model_url=BERT_TFHUB_URL):\n",
    "    # Load the pre-trained BERT model as layer in Keras\n",
    "    bert_layer = hub.KerasLayer(\n",
    "        handle=model_url,\n",
    "        trainable=False)\n",
    "    return bert_layer\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "def get_model(tf_transform_output, max_seq_length=512):\n",
    "\n",
    "    # dynamically create inputs for all outputs of our transform graph\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec()  \n",
    "    feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "    inputs = {\n",
    "        key: tf.keras.layers.Input(shape=(max_seq_length), name=key, dtype=tf.int64)\n",
    "            for key in feature_spec.keys()\n",
    "    }\n",
    "\n",
    "    input_word_ids = tf.cast(inputs[\"input_word_ids\"], dtype=tf.int32)\n",
    "    input_mask = tf.cast(inputs[\"input_mask\"], dtype=tf.int32)\n",
    "    input_type_ids = tf.cast(inputs[\"input_type_ids\"], dtype=tf.int32)\n",
    "\n",
    "    bert_layer = load_bert_layer()\n",
    "    encoder_inputs = dict(\n",
    "        input_word_ids=tf.reshape(input_word_ids, (-1, max_seq_length)),\n",
    "        input_mask=tf.reshape(input_mask, (-1, max_seq_length)),\n",
    "        input_type_ids=tf.reshape(input_type_ids, (-1, max_seq_length)),\n",
    "    )\n",
    "    outputs = bert_layer(encoder_inputs)\n",
    "    \n",
    "    # Add additional layers depending on your problem\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(outputs[\"pooled_output\"])\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    keras_model = tf.keras.Model(\n",
    "        inputs=[\n",
    "                inputs['input_word_ids'], \n",
    "                inputs['input_mask'], \n",
    "                inputs['input_type_ids']], \n",
    "        outputs=pred)\n",
    "    keras_model.compile(loss='binary_crossentropy', \n",
    "                        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                        metrics=['accuracy']\n",
    "                        )\n",
    "    return keras_model\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _input_fn(file_pattern: Text,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 32) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "      file_pattern: input tfrecord file pattern.\n",
    "      tf_transform_output: A TFTransformOutput.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=_gzip_reader_fn,\n",
    "        label_key=LABEL_KEY)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "      fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 32)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 32)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = get_model(tf_transform_output=tf_transform_output)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps)\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "            _get_serve_tf_examples_fn(model,\n",
    "                                      tf_transform_output).get_concrete_function(\n",
    "                                          tf.TensorSpec(\n",
    "                                              shape=[None],\n",
    "                                              dtype=tf.string,\n",
    "                                              name='examples')),\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "b4n7fkCbnvHW",
    "outputId": "bf8d0365-04dc-42f8-983c-74579031fe28"
   },
   "outputs": [],
   "source": [
    "# NOTE: Adjust the number of training and evaluation steps when training in an production setup\n",
    "TRAINING_STEPS = 5\n",
    "EVALUATION_STEPS = 5\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=os.path.abspath(\"trainer.py\"),\n",
    "    # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "LD2kK5XQenDL",
    "outputId": "3497973d-407f-4086-d312-086743e7aa8a"
   },
   "outputs": [],
   "source": [
    "# model_resolver = tfx.dsl.Resolver(\n",
    "#     instance_name='latest_blessed_model_resolver',\n",
    "#     resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "#     model=Channel(type=Model),\n",
    "#     model_blessing=Channel(type=ModelBlessing))\n",
    "\n",
    "model_resolver = resolver.Resolver(\n",
    "      strategy_class=tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy ,\n",
    "      model=tfx.types.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "      model_blessing=tfx.types.Channel(\n",
    "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "              'latest_blessed_model_resolver')\n",
    "\n",
    "context.run(model_resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgH50dYW5C2T"
   },
   "source": [
    "## TensorFlow Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "NVOPbS9Te6MW",
    "outputId": "273dd605-c9c5-49f5-8cfd-06aa9cdad194"
   },
   "outputs": [],
   "source": [
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "    slicing_specs=[tfma.SlicingSpec()],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "            tfma.MetricConfig(\n",
    "                class_name='CategoricalAccuracy',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value': 0.5}),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value': -1e-2})))\n",
    "        ])\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],\n",
    "    eval_config=eval_config\n",
    ")\n",
    "\n",
    "context.run(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD3Q8gnznAnT",
    "outputId": "d1af5553-d1fb-4d4e-ef87-f99b20e0b1f0"
   },
   "outputs": [],
   "source": [
    "# Check the blessing\n",
    "!ls {evaluator.outputs['blessing'].get()[0].uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f4Z0vJWOIyk"
   },
   "source": [
    "## Model Export for Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxxXrsdebY63"
   },
   "outputs": [],
   "source": [
    "!mkdir /serving_model_dir\n",
    "\n",
    "serving_model_dir = \"/serving_model_dir\"\n",
    "\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=evaluator.outputs['blessing'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=serving_model_dir)))\n",
    "\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWni3fVVafDa"
   },
   "source": [
    "## Test your Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTi19Ojrbumq"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "push_uri = pusher.outputs.model_push.get()[0].uri\n",
    "latest_version = max(os.listdir(push_uri))\n",
    "latest_version_path = os.path.join(push_uri, latest_version)\n",
    "loaded_model = tf.saved_model.load(latest_version_path)\n",
    "\n",
    "example_str = b\"I am so sad today, I wish this movie was never released !!!\"\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'text': _bytes_feature(example_str)}))\n",
    "\n",
    "serialized_example = example.SerializeToString()\n",
    "f = loaded_model.signatures[\"serving_default\"]\n",
    "print(f(tf.constant([serialized_example])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/\n",
    "def create_final_pipeline(\n",
    "    pipeline_name: Text,\n",
    "    root_path: Text,\n",
    "    data_path: Text,\n",
    "    # training_params: Dict[Text, Text],\n",
    "    transform_module: Text,\n",
    "    model_module: Text,\n",
    "    project_id = \"\",\n",
    "    region = \"\",\n",
    "    use_vertex_ai = False,\n",
    "    beam_pipeline_args = None, ## ['--direct_num_workers=%d' % 0],\n",
    "    vertex_trainer_image = \"\",\n",
    "    train_steps = 5,\n",
    "    eval_steps = 5,    \n",
    ") -> pipeline.Pipeline:\n",
    "\n",
    "    TRAINING_STEPS = train_steps\n",
    "    EVALUATION_STEPS = eval_steps\n",
    "    \n",
    "    _pipeline_root = os.path.join(root_path, 'pipelines');      # Join ~/tfx/pipelines/\n",
    "    _metadata_db_root = os.path.join(root_path, 'metadata.db');    # Join ~/tfx/metadata.db\n",
    "    _log_root = os.path.join(root_path, 'logs');\n",
    "    _model_root = os.path.join(root_path, 'model');\n",
    "    _serving_model_dir = os.path.join(root_path, 'serving_model')\n",
    "\n",
    "    # Full pipeline\n",
    "    # example_gen = CsvExampleGen(input_base=data_path)\n",
    "    output = example_gen_pb2.Output(\n",
    "             split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "                 example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=45),\n",
    "                 example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=5)\n",
    "             ]))\n",
    "    \n",
    "    # Load the data from our prepared TFDS folder\n",
    "    # data_path = \"tfds/imdb_reviews/plain_text/1.0.0/\"\n",
    "    example_gen = ImportExampleGen(input_base=data_path, \n",
    "                                   output_config=output)\n",
    "\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "    schema_gen = SchemaGen(\n",
    "                    statistics=statistics_gen.outputs['statistics'],\n",
    "                    infer_feature_shape=True)\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "                    statistics=statistics_gen.outputs['statistics'],\n",
    "                    schema=schema_gen.outputs['schema'])\n",
    "\n",
    "    # transform = Transform(\n",
    "    #     examples=example_gen.outputs['examples'],\n",
    "    #     schema=schema_gen.outputs['schema'],\n",
    "    #     force_tf_compat_v1=True,\n",
    "    #     module_file=transform_module)\n",
    "    \n",
    "    transform = Transform(\n",
    "            examples=example_gen.outputs['examples'],\n",
    "            schema=schema_gen.outputs['schema'],\n",
    "            module_file=transform_module)\n",
    "\n",
    "    ##Don't use Vertex AI services\n",
    "    if use_vertex_ai == False:\n",
    "        # tuner = Tuner(\n",
    "        #         module_file=model_module,\n",
    "        #         # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        #         examples=transform.outputs['transformed_examples'],\n",
    "        #         train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=100), ##goes as train_steps / steps_per_epoch\n",
    "        #         eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=3), ##goes as eval_steps\n",
    "        #         transform_graph=transform.outputs['transform_graph'],\n",
    "        #         custom_config=({\"epochs\": 10}))\n",
    "        \n",
    "        # trainer = Trainer(\n",
    "        #             custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        #             module_file=model_module,\n",
    "        #             transformed_examples=transform.outputs['transformed_examples'],\n",
    "        #             # schema=schema_importer.outputs.result,\n",
    "        #             transform_graph=transform.outputs['transform_graph'],\n",
    "        #             hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "        #             custom_config=training_params,\n",
    "        #             train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=100),\n",
    "        #             eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=3))\n",
    "        trainer = Trainer(\n",
    "            module_file=os.path.abspath(\"trainer.py\"),\n",
    "            # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "            examples=transform.outputs['transformed_examples'],\n",
    "            transform_graph=transform.outputs['transform_graph'],\n",
    "            schema=schema_gen.outputs['schema'],\n",
    "            train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
    "            eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS))\n",
    "        \n",
    "        model_resolver = resolver.Resolver(\n",
    "              strategy_class=tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy ,\n",
    "              model=tfx.types.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "              model_blessing=tfx.types.Channel(\n",
    "                  type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "                      'latest_blessed_model_resolver')\n",
    "        \n",
    "        eval_config = tfma.EvalConfig(\n",
    "            model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "            slicing_specs=[tfma.SlicingSpec()],\n",
    "            metrics_specs=[\n",
    "                tfma.MetricsSpec(metrics=[\n",
    "                    tfma.MetricConfig(\n",
    "                        class_name='CategoricalAccuracy',\n",
    "                        threshold=tfma.MetricThreshold(\n",
    "                            value_threshold=tfma.GenericValueThreshold(\n",
    "                                lower_bound={'value': 0.3}),\n",
    "                            change_threshold=tfma.GenericChangeThreshold(\n",
    "                                direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                                absolute={'value': -1e-2})))\n",
    "                ])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        evaluator = Evaluator(\n",
    "            examples=example_gen.outputs['examples'],\n",
    "            model=trainer.outputs['model'],\n",
    "            baseline_model=model_resolver.outputs['model'],\n",
    "            eval_config=eval_config\n",
    "        )\n",
    "        \n",
    "        ## Push without evaluation for now\n",
    "        pusher = Pusher(\n",
    "            model=trainer.outputs['model'],\n",
    "            # model_blessing=evaluator.outputs['blessing'],\n",
    "            push_destination=pusher_pb2.PushDestination(\n",
    "                filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                    base_directory=_serving_model_dir))\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        ##Trainer\n",
    "        vertex_trainer_job_spec = {\n",
    "              'project': project_id,\n",
    "              'worker_pool_specs': [{\n",
    "                  'machine_spec': {\n",
    "                      'machine_type': 'n1-standard-4',\n",
    "                      'accelerator_type': 'NVIDIA_TESLA_K80', ##GPU Training\n",
    "                      'accelerator_count': 1,\n",
    "                  },\n",
    "                  'replica_count': 1,\n",
    "                  'container_spec': {\n",
    "                      'image_uri': vertex_trainer_image,\n",
    "                  },\n",
    "              }],\n",
    "          }\n",
    "        \n",
    "        trainer = gcp_trainer.Trainer(\n",
    "                      module_file=model_module,\n",
    "                      examples=transform.outputs['transformed_examples'],\n",
    "                      train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=TRAINING_STEPS),\n",
    "                      eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=EVALUATION_STEPS),\n",
    "                      # hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "                      transform_graph=transform.outputs['transform_graph'],\n",
    "                      custom_config={\n",
    "                          ENABLE_VERTEX_KEY:\n",
    "                              True,\n",
    "                          VERTEX_REGION_KEY:\n",
    "                              region,\n",
    "                          tfx.extensions.google_cloud_ai_platform.trainer.executor.TRAINING_ARGS_KEY:\n",
    "                              vertex_trainer_job_spec,\n",
    "                          # \"epochs\": training_params[\"epochs\"]\n",
    "                      })\n",
    "\n",
    "        # NEW: Pushes the model to Vertex AI Endpoints\n",
    "        # NEW: Configuration for pusher.\n",
    "        vertex_serving_spec = {\n",
    "          'project_id': project_id,\n",
    "          'endpoint_name': 'tfx_bertclassif_ep',\n",
    "          'machine_type': 'n1-standard-4',\n",
    "        }\n",
    "        \n",
    "        serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest'\n",
    "        pusher = gcp_pusher.Pusher(\n",
    "              model=trainer.outputs['model'],\n",
    "              custom_config={\n",
    "                  ENABLE_VERTEX_KEY:\n",
    "                      True,\n",
    "                  VERTEX_REGION_KEY:\n",
    "                      region,\n",
    "                  tfx.extensions.google_cloud_ai_platform.constants.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "                      serving_image,\n",
    "                  tfx.extensions.google_cloud_ai_platform.constants.SERVING_ARGS_KEY:\n",
    "                    vertex_serving_spec,\n",
    "              })\n",
    "    \n",
    "    # This pipeline obj carries the business logic of the pipeline, but no runner-specific information\n",
    "    # was included.    \n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=  pipeline_name,\n",
    "        pipeline_root=  root_path,\n",
    "        components=[\n",
    "            example_gen, \n",
    "            statistics_gen, \n",
    "            schema_gen, \n",
    "            example_validator,\n",
    "            transform, \n",
    "            trainer,\n",
    "            # model_resolver,\n",
    "            # evaluator,\n",
    "            pusher\n",
    "        ],\n",
    "        metadata_connection_config = metadata.sqlite_metadata_connection_config(_metadata_db_root),\n",
    "        enable_cache=False,\n",
    "        beam_pipeline_args=beam_pipeline_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Run full pipeline locally\n",
    "from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n",
    "\n",
    "##Define all paths\n",
    "_tfx_root = os.path.join(os.getcwd(), 'tfx')\n",
    "_transform_module_file = 'transform.py'\n",
    "_model_trainer_module_file = 'trainer.py'\n",
    "\n",
    "#Config params\n",
    "# training_params = {\"epochs\": 20}\n",
    "\n",
    "#Create and run pipeline\n",
    "p_ = create_final_pipeline(root_path = _tfx_root, \n",
    "                           pipeline_name=\"local_pipeline\", \n",
    "                           data_path=os.path.join(os.getcwd(), \"tfds/imdb_reviews/plain_text/1.0.0/\"),\n",
    "                           # training_params=training_params,\n",
    "                           transform_module=os.path.abspath(_transform_module_file),\n",
    "                           model_module=os.path.abspath(_model_trainer_module_file)\n",
    "                          )\n",
    "\n",
    "## UNCOMMENT: To test Local pipeline( only for DEBUGGING )\n",
    "LocalDagRunner().run(p_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFX Pipeline on GCP deployment - Pre-steps\n",
    "\n",
    "- Build Dockerfile for TFX base image\n",
    "- Upload image to GAR\n",
    "- Copy code and data to GCS for usage in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "##Build Dockerfile\n",
    "FROM tensorflow/tfx:1.12.0\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Docker image\n",
    "!docker build -t us-central1-docker.pkg.dev/hasanrafiq-test-331814/tfx-images/latest:latest -f Dockerfile .\n",
    "\n",
    "#Get access to write to GAR\n",
    "!yes | gcloud auth configure-docker us-central1-docker.pkg.dev\n",
    "\n",
    "## Push Docker image to GAR\n",
    "!docker push us-central1-docker.pkg.dev/hasanrafiq-test-331814/tfx-images/latest:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# move necessary files to GCS\n",
    "PROJECT_ID=\"hasanrafiq-test-331814\" \n",
    "BUCKET_NAME=\"gs://gcs-${PROJECT_ID}\"\n",
    "\n",
    "gsutil cp trainer.py ${BUCKET_NAME}/tfx/bert-classif/code/\n",
    "gsutil cp transform.py ${BUCKET_NAME}/tfx/bert-classif/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# move necessary files to GCS\n",
    "PROJECT_ID=\"hasanrafiq-test-331814\" \n",
    "BUCKET_NAME=\"gs://gcs-${PROJECT_ID}\"\n",
    "\n",
    "gsutil cp tfds/imdb_reviews/plain_text/1.0.0/* ${BUCKET_NAME}/tfx/bert-classif/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End pipeline - Vertex Pipeline( w/o GCP ML services )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'tfx-bert-classif'\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '-pipeline.json'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "\n",
    "PROJECT_ID = \"hasanrafiq-test-331814\" \n",
    "BUCKET_NAME = \"gs://gcs-\" + PROJECT_ID\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}\"\n",
    "\n",
    "TRANSFORM_MODULE = f\"{BUCKET_NAME}/tfx/bert-classif/code/transform.py\"\n",
    "MODEL_MODULE = f\"{BUCKET_NAME}/tfx/bert-classif/code/trainer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "# training_params = {\"epochs\": 20}\n",
    "use_dataflow_services = False  ##If need DataFlow for dataprocessing\n",
    "\n",
    "##Define all paths\n",
    "_tfx_root = os.path.join(PIPELINE_ROOT, 'tfx')\n",
    "\n",
    "##Pickup runner as per value of \"use_cloud_services\"\n",
    "runners = {True : 'DataflowRunner', False : 'DirectRunner'}\n",
    "\n",
    "##Dataflow configuration\n",
    "beam_pipeline_args = [\n",
    "    f'--runner={runners[use_dataflow_services]}',\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--temp_location={BUCKET_NAME}/tmp',\n",
    "    f'--region={GOOGLE_CLOUD_REGION}',\n",
    "    f'--staging_location={BUCKET_NAME}/staging'\n",
    "]\n",
    "\n",
    "##Set Vertex Runner\n",
    "# KFP V2 Runner - Vertex\n",
    "custom_container = \"us-central1-docker.pkg.dev/hasanrafiq-test-331814/tfx-images/latest:latest\"\n",
    "gcp_runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
    "            default_image=custom_container),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "\n",
    "#KFP V1 Runner - AI Platform\n",
    "# runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n",
    "#       kubeflow_metadata_config=kubeflow_dag_runner.get_default_kubeflow_metadata_config())\n",
    "# gcp_runner = kubeflow_dag_runner.KubeflowDagRunner(config=runner_config)\n",
    "\n",
    "p_ = create_final_pipeline(root_path = _tfx_root, \n",
    "                           pipeline_name=PIPELINE_NAME, \n",
    "                           data_path=os.path.join(_tfx_root, \"bert-classif/data\"),\n",
    "                           # training_params=training_params,\n",
    "                           beam_pipeline_args=beam_pipeline_args,\n",
    "                           transform_module=TRANSFORM_MODULE,\n",
    "                           model_module=MODEL_MODULE,\n",
    "                           project_id=PROJECT_ID,\n",
    "                           region=GOOGLE_CLOUD_REGION,\n",
    "                           use_vertex_ai=False\n",
    "                          )\n",
    "\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "_ = gcp_runner.run(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_infra: no_execute\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "aiplatform.init(location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME,\n",
    "                                pipeline_root=os.path.join(PIPELINE_ROOT, 'pipeline_root'))\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End pipeline - Vertex Pipeline( with GCP ML services )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "# training_params = {\"epochs\": 20}\n",
    "use_dataflow_services = False  ##If need DataFlow for dataprocessing\n",
    "\n",
    "##Define all paths\n",
    "_tfx_root = os.path.join(PIPELINE_ROOT, 'tfx')\n",
    "\n",
    "##Pickup runner as per value of \"use_cloud_services\"\n",
    "runners = {True : 'DataflowRunner', False : 'DirectRunner'}\n",
    "\n",
    "##Dataflow configuration\n",
    "beam_pipeline_args = [\n",
    "    f'--runner={runners[use_dataflow_services]}',\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--temp_location={BUCKET_NAME}/tmp',\n",
    "    f'--region={GOOGLE_CLOUD_REGION}',\n",
    "    f'--staging_location={BUCKET_NAME}/staging'\n",
    "]\n",
    "\n",
    "##Set Vertex Runner\n",
    "# KFP V2 Runner - Vertex\n",
    "custom_container = \"us-central1-docker.pkg.dev/hasanrafiq-test-331814/tfx-images/latest:latest\"\n",
    "gcp_runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
    "            default_image=custom_container),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "\n",
    "#KFP V1 Runner - AI Platform\n",
    "# runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n",
    "#       kubeflow_metadata_config=kubeflow_dag_runner.get_default_kubeflow_metadata_config())\n",
    "# gcp_runner = kubeflow_dag_runner.KubeflowDagRunner(config=runner_config)\n",
    "\n",
    "p_ = create_final_pipeline(root_path = _tfx_root, \n",
    "                           pipeline_name=PIPELINE_NAME, \n",
    "                           data_path=os.path.join(_tfx_root, \"bert-classif/data\"),\n",
    "                           # training_params=training_params,\n",
    "                           beam_pipeline_args=beam_pipeline_args,\n",
    "                           transform_module=TRANSFORM_MODULE,\n",
    "                           model_module=MODEL_MODULE,\n",
    "                           project_id=PROJECT_ID,\n",
    "                           region=GOOGLE_CLOUD_REGION,\n",
    "                           use_vertex_ai=True,\n",
    "                           vertex_trainer_image=custom_container,\n",
    "                           train_steps = 20,\n",
    "                           eval_steps = 5\n",
    "                          )\n",
    "\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "_ = gcp_runner.run(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_infra: no_execute\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "aiplatform.init(location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME,\n",
    "                                pipeline_root=os.path.join(PIPELINE_ROOT, 'pipeline_root'))\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TFX_Pipeline_for_Bert_Preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
